{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NWB conversion tutorial \n",
    "\n",
    "This notebook contains a step-by-step tutorial for converting your NeuroPAL structural images and calcium activity time series to the NWB format and uploading to the DANDI archive. One NWB file will contain all of the raw data, processed data, and metadata associated with one trial of an experiment in one worm.\n",
    "\n",
    "We will show an example of NWB conversion using a dataset collected by Raymond Dunn in the FOCO lab at UCSF. This dataset contains the raw neuroPAL and GCaMP data saved as tif files and neuron segmentation results as well as GCaMP quantification data saved as csv files. This tutorial will also work with data stored in different formats, but you may have to add some additional or different steps to load that data into the proper format for NWB files.\n",
    "\n",
    "This tutorial should cover most basic applications of optical physiology in *C. elegans*. NWB can also flexibly incorporate many other types of data as well. See https://pynwb.readthedocs.io/en/stable/tutorials/index.html for documentation and tutorials on other types of data as well.\n",
    "\n",
    "We hope for this process to be a collaborative effort for the worm community. Please feel free to reach out to daniel.sprague@ucsf.edu with any questions or suggestions for changes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data used in this tutorial\n",
    "\n",
    "This box folder https://ucsf.box.com/s/8kbdfywefcfsn4pfextrzcr25az1vmuj contains a video explaining this tutorial step by step. It also contains a data folder with the example data that is used in this tutorial. Please download the data folder to your local drive to follow along in the tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup\n",
    "\n",
    "Below are the necessary imports you need to work with your data and interface with NWB.\n",
    "\n",
    "You can use the setup.py file found in this repository to automatically add all necessary packages to your environment. Create a fresh Python virtual environment and run \"pip install .\" in the command line while in the root directory of this Git repository. You will also need to run \"ipython kernel install --user --name=*name of your environment*\" within your virtual environment to use the environment in Jupyter notebook.\n",
    "\n",
    "There are a number of python environment management platforms out there. Most people in our lab use anaconda https://docs.conda.io/projects/conda/en/stable/. \n",
    "\n",
    "We include the bare minimum packages and imports for the example shown in this tutorial. If you have different file types or want to add additional types of data to your NWB file, you may need additional imports as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "from hdmf.backends.hdf5.h5_utils import H5DataIO\n",
    "from hdmf.container import Container\n",
    "from hdmf.data_utils import DataChunkIterator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pynwb import load_namespaces, get_class, register_class, NWBFile, TimeSeries, NWBHDF5IO\n",
    "from pynwb.file import MultiContainerInterface, NWBContainer, Device, Subject\n",
    "from pynwb.ophys import ImageSeries, OnePhotonSeries, OpticalChannel, ImageSegmentation, PlaneSegmentation, Fluorescence, DfOverF, CorrectedImageStack, MotionCorrection, RoiResponseSeries, ImagingPlane\n",
    "from pynwb.core import NWBDataInterface\n",
    "from pynwb.epoch import TimeIntervals\n",
    "from pynwb.behavior import SpatialSeries, Position\n",
    "from pynwb.image import ImageSeries\n",
    "import scipy.io as sio\n",
    "import skimage.io as skio\n",
    "from tifffile import TiffFile\n",
    "\n",
    "# ndx_mulitchannel_volume is the novel NWB extension for multichannel optophysiology in C. elegans\n",
    "from ndx_multichannel_volume import CElegansSubject, OpticalChannelReferences, OpticalChannelPlus, ImagingVolume, VolumeSegmentation, MultiChannelVolume, MultiChannelVolumeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to location of NWB_tutorial folder on your local system\n",
    "datapath = '/Users/danielysprague/foco_lab/data/NWB_tutorial/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the NWB file and adding basic metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NWBFile object is the base object which all other objects will be added to. This object holds \n",
    "metadata about the experiment, the specific trial, and the lab.\n",
    "\n",
    "We show an example with the minimum metadata that should be included in any NWB file. Other optional\n",
    "metadata fields can be found at https://pynwb.readthedocs.io/en/stable/pynwb.file.html#pynwb.file.NWBFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbfile = NWBFile(\n",
    "    session_description = 'Add a description for the experiment/session. Can be just long form text',\n",
    "    #Can use any identity marker that is specific to an individual trial. We use date-time to specify trials\n",
    "    identifier = '20230322-21-41-10',\n",
    "    #Specify date and time of trial. Datetime entries are in order Year, Month, Day, Hour, Minute, Second. Not all entries are necessary\n",
    "    session_start_time = datetime(2023, 3, 22, 21, 41, 10, tzinfo=tz.gettz(\"US/Pacific\")),\n",
    "    lab = 'FOCO lab',\n",
    "    institution = 'UCSF',\n",
    "    related_publications = ''\n",
    ")\n",
    "\n",
    "nwbfile #in the latest version of PyNWB, you can output the PyNWB object in Jupyter notebook to easily visualize all of the fields contained within the object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'subject' object is added onto the nwbfile object and contains the metadata associated with the subject of the\n",
    "experiment. This should be specific to the worm that you are using for this trial of the experiment.\n",
    "\n",
    "Please reach out if there are other metadata fields that you think should be included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwbfile.subject = CElegansSubject(\n",
    "    #This is the same as the NWBFile identifier for us, but does not have to be. It should just identify the subject for this trial uniquely.\n",
    "    subject_id = '20230322-21-41-10',\n",
    "    #Age is optional but should be specified in ISO 8601 duration format similarly to what is shown here for growth_stage_time\n",
    "    #age = pd.Timedelta(hours=2, minutes=30).isoformat(),\n",
    "    #Date of birth is a required field but if you do not know or if it's not relevant, you can just use the current date or the date of the experiment\n",
    "    date_of_birth = datetime(2023, 3, 20, tzinfo=tz.gettz(\"US/Pacific\")), \n",
    "    #Specify growth stage of worm - should be one of two-fold, three-fold, L1-L4, YA, OA, dauer, post-dauer L4, post-dauer YA, post-dauer OA\n",
    "    growth_stage = 'YA',\n",
    "    #Optional: specify time the worm has spent in current growth stage\n",
    "    growth_stage_time=pd.Timedelta(hours=2, minutes=30).isoformat(),\n",
    "    #Specify temperature at which animal was cultivated\n",
    "    cultivation_temp = 20.,\n",
    "    description = \"free form text description, can include whatever you want here\",\n",
    "    #Currently using the ontobee species link until NWB adds support for C. elegans\n",
    "    species  =  \"http://purl.obolibrary.org/obo/NCBITaxon_6239\",\n",
    "    #Currently just using O for other until support added for other gender specifications\n",
    "    sex = \"O\", \n",
    "    strain = \"FC128\"\n",
    ")\n",
    "\n",
    "nwbfile.subject"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The device object is where you specify the metadata of the type of microscope/data acquisition device\n",
    "used. If there are multiple devices you are using, you can call nwbfile.create_device multiple times. \n",
    "Just make sure to give each device a different name and assign it to a different variable. Here we define \n",
    "the microscope used to acquire both our NeuroPAL structural images and our calcium imaging time series data.\n",
    "\n",
    "This device object will be attached to your data objects to show how they were acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = nwbfile.create_device(\n",
    "    name = \"Spinning disk confocal\",\n",
    "    description = \"Leica DMi8 Inverted Microscope with Yokogawa CSU-W1 SoRA, 40x WI objective 1.1 NA\",\n",
    "    manufacturer = \"Leica, Yokagawa\"\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding NeuroPAL images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the ImagingVolume object. This object contains the metadata associated with the image acquisition including\n",
    "the device used, the scale of the image in each dimension, and the excitation laser/emission filters/fluorophores used for \n",
    "each channel in the image.\n",
    "\n",
    "You can create multiple ImagingVolume objects for each separate type of imaging data that you acquired. As with devices, just\n",
    "make sure each ImagingVolume has a unique name and variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Channels is a list of tuples where each tuple contains the fluorophore used, the specific emission filter used, and a short description\n",
    "#structured as \"excitation wavelength - emission filter center point- width of emission filter in nm\"\n",
    "#Make sure this list is in the same order as the channels in your data\n",
    "channels = [(\"mTagBFP2\", \"Chroma ET 460/50\", \"405-460-50m\"), (\"CyOFP1\", \"Chroma ET 605/70\",\"488-605-70m\"), (\"CyOFP1-high filter\", \"Chroma ET 700/75\",\"488-700-75m\"), (\"GFP-GCaMP\", \"Chroma ET 525/50\",\"488-525-50m\"), (\"mNeptune 2.5\", \"Chroma ET 700/75\", \"561-700-75m\"), (\"Tag RFP-T\", \"Chroma ET 605/70\", \"561-605-70m\"), (\"mNeptune 2.5-far red\", \"Chroma ET 700/75\", \"639-700-75m\")]\n",
    "\n",
    "\n",
    "OptChannels = []\n",
    "OptChanRefData = []\n",
    "#The loop below takes the list of channels and converts it into a list of OpticalChannelPlus objects which hold the metadata\n",
    "#for the optical channels used in the experiment\n",
    "for fluor, des, wave in channels:\n",
    "    excite = float(wave.split('-')[0])\n",
    "    emiss_mid = float(wave.split('-')[1])\n",
    "    emiss_range = float(wave.split('-')[2][:-1])\n",
    "    OptChan = OpticalChannelPlus(\n",
    "        name = fluor,\n",
    "        description = des,\n",
    "        excitation_lambda = excite,\n",
    "        excitation_range = [excite-1.5, excite+1.5],\n",
    "        emission_range = [emiss_mid-emiss_range/2, emiss_mid+emiss_range/2],\n",
    "        emission_lambda = emiss_mid\n",
    "    )\n",
    "\n",
    "    OptChannels.append(OptChan)\n",
    "    OptChanRefData.append(wave)\n",
    "\n",
    "\n",
    "#This object just contains references to the order of channels because OptChannels does not preserve ordering by itself\n",
    "OpticalChannelRefs = OpticalChannelReferences(\n",
    "    name = 'OpticalChannelRefs',\n",
    "    channels = OptChanRefData\n",
    ")\n",
    "\n",
    "ImagingVol = ImagingVolume(\n",
    "    name= 'NeuroPALImVol',\n",
    "    #Add connections to the OptChannels and OpticalChannelRefs objects\n",
    "    optical_channel_plus = OptChannels,\n",
    "    order_optical_channels = OpticalChannelRefs,\n",
    "    #Free form description of what is being imaged in this volume\n",
    "    description = 'NeuroPAL image of C. elegans brain',\n",
    "    #Reference the device created earlier that was used to acquire this data\n",
    "    device = device,\n",
    "    #Specifies where in the C. elegans body the image is being taken of\n",
    "    location = \"Head\",\n",
    "    #Specifies the voxel spacing in x, y, z respectively. The values specified should be how many micrometers of physical\n",
    "    #distance are covered by a single pixel in each dimension\n",
    "    grid_spacing = [0.3208, 0.3208, 0.75],\n",
    "    grid_spacing_unit = 'micrometers',\n",
    "    #Origin coords, origin coords unit, and reference frames are carry over fields from other model organisms where you \n",
    "    #are likely only looking at a small portion of the brain. These fields are unfortunately required but feel free to put\n",
    "    #whatever feels right here\n",
    "    origin_coords = [0, 0, 0],\n",
    "    origin_coords_unit = \"micrometers\",\n",
    "    reference_frame = \"Worm head\"\n",
    ")\n",
    "\n",
    "nwbfile.add_imaging_plane(ImagingVol) #add this ImagingVol to the nwbfile \n",
    "\n",
    "ImagingVol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpticalChannelRefs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the MultiChannelVolume object which is where our volumetric images (e.g. NeuroPAL)\n",
    "will be stored.\n",
    "\n",
    "The data that is being stored here will be a 4D array: X, Y, Z, C. Each entry of the array will be the grey count\n",
    "value for the pixel located at a specific X, Y, Z location in a specific channel.\n",
    "\n",
    "We use tif files to store this data in our lab so the example here will be shown with tifs, but this can work \n",
    "with any file type that can be loaded and converted into a numpy array.\n",
    "\n",
    "Make sure you know the order of the dimensions of your data when inputting here. The order should be X, Y, Z, C. The channels must be the last dimension, but what\n",
    "you define as X, Y, Z is somewhat arbitrary. For our imaging conditions, we define Z to be the slices and X, Y to refer to the \n",
    "dimensions of each slice. Then X is the longer of those two dimensions. It is up to you how you want to define these dimensions, \n",
    "just make sure that the order of the dimensions you choose is consistent with the order of dimensions for the pixel resolution you \n",
    "input here and the segmentation masks which we will define later.\n",
    "\n",
    "Again, you can have any number of these images in one NWBfile, just make sure they are all named uniquely and that you provide\n",
    "a reference to the correct Imaging Volume object for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file = datapath+ 'NP_im_raw.tif'\n",
    "\n",
    "#Data is in order C, Z, Y, X when reading in \n",
    "data = skio.imread(raw_file)\n",
    "print(data.shape)\n",
    "data = np.transpose(data) #Tranpose data to proper order\n",
    "print(data.shape)\n",
    "\n",
    "RGBW_channels = [0,1,4,6]\n",
    "\n",
    "Image = MultiChannelVolume(\n",
    "    name = 'NeuroPALImageRaw',\n",
    "    #This is the same OpticalChannelRefs used in the associated Imaging Volume\n",
    "    order_optical_channels = OpticalChannelRefs,\n",
    "    description = 'free form description of image',\n",
    "    #Specifies which channels in the image are associated with the RGBW channels - should be a list of channel indices as shown above\n",
    "    RGBW_channels = RGBW_channels,\n",
    "    #This is the raw data numpy array that we loaded above\n",
    "    data = data,\n",
    "    #This is a reference to the Imaging Volume object we defined previously\n",
    "    imaging_volume = ImagingVol\n",
    ")\n",
    "\n",
    "Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of adding another image, I will also create an image object and associated image volume\n",
    "for a version of the data that has undergone some basic pre-processing steps. Note that we create a new\n",
    "imaging volume because this processed image only has 4 channels associated with the RGBW channels, so we\n",
    "need a separate imaging volume to describe the order of the channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file = datapath+'NP_im_proc.tif'\n",
    "\n",
    "proc_data = np.transpose(skio.imread(processed_file),[2,1,0,3])\n",
    "\n",
    "print(proc_data.shape)\n",
    "\n",
    "proc_channels = [channels[i] for i in RGBW_channels]\n",
    "\n",
    "ProcOptChannels = []\n",
    "ProcOptChanRefData = []\n",
    "for fluor, des, wave in proc_channels:\n",
    "    excite = float(wave.split('-')[0])\n",
    "    emiss_mid = float(wave.split('-')[1])\n",
    "    emiss_range = float(wave.split('-')[2][:-1])\n",
    "    OptChan = OpticalChannelPlus(\n",
    "        name = fluor,\n",
    "        description = des,\n",
    "        excitation_lambda = excite,\n",
    "        excitation_range = [excite-1.5, excite+1.5],\n",
    "        emission_range = [emiss_mid-emiss_range/2, emiss_mid+emiss_range/2],\n",
    "        emission_lambda = emiss_mid\n",
    "    )\n",
    "\n",
    "    ProcOptChannels.append(OptChan)\n",
    "    ProcOptChanRefData.append(wave)\n",
    "\n",
    "\n",
    "ProcOpticalChannelRefs = OpticalChannelReferences(\n",
    "    name = 'OpticalChannelRefs',\n",
    "    channels = ProcOptChanRefData\n",
    ")\n",
    "\n",
    "ProcImagingVol = ImagingVolume(\n",
    "    name= 'ProcessedImVol',\n",
    "    optical_channel_plus = ProcOptChannels,\n",
    "    order_optical_channels = ProcOpticalChannelRefs,\n",
    "    description = 'NeuroPAL image of C. elegans brain',\n",
    "    device = device,\n",
    "    location = \"Head\",\n",
    "    grid_spacing = [0.3208, 0.3208, 0.75],\n",
    "    grid_spacing_unit = 'micrometers',\n",
    "    origin_coords = [0, 0, 0],\n",
    "    origin_coords_unit = \"micrometers\",\n",
    "    reference_frame = \"Worm head\"\n",
    ")\n",
    "\n",
    "ProcImage = MultiChannelVolume(\n",
    "    name = 'ProcessedImage',\n",
    "    order_optical_channels = ProcOpticalChannelRefs,\n",
    "    description = 'Can describe the specific pre-processing steps taken here',\n",
    "    RGBW_channels = [0,1,2,3],\n",
    "    data = proc_data,\n",
    "    imaging_volume = ProcImagingVol\n",
    ")\n",
    "\n",
    "nwbfile.add_imaging_plane(ProcImagingVol)\n",
    "\n",
    "ProcImage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Adding segmentation and ID results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add the results of a neuron segmentation to the NWB file using the volume segmentation object. The process described here can be used to add other types of ROIs\n",
    "as well. As before, just be sure to properly name and describe each of the segmentations you create.\n",
    "\n",
    "There are two ways of doing this: using an image mask or a voxel mask. I will provide examples of both approaches here, but for the purposes of neuron segmentation, you \n",
    "would most likely be best off using the voxel mask approach. Note that each PlaneSegmentation object should have either an image mask or voxel mask and not both.\n",
    "\n",
    "With image mask, each ROI will be represented by an array which is the same size as the original image where non-ROI voxels have value 0 and ROI voxels have non-zero value.\n",
    "You should use image mask for ROIs which cover large portions of the image. For example, this would be useful if you were providing an ROI mask for where the body of the worm\n",
    "is compared to background.\n",
    "\n",
    "With voxel mask, each ROI is represented by a list of voxels that belong to that ROI. Each entry in this list should have [x, y, z, weight]. The weight should be a float value\n",
    "which can have any meaning (i.e. could be the grey count of the voxel or a confidence level for that voxel being a member of that ROI). \n",
    "\n",
    "\n",
    "Labels should contain an ordered list of labels for the ROIs you include in this segmentation. Labels are optional but if you do include them make sure the list 'labels' is the same length as the number of ROIs you have, using the empty string '' where there is not an associated label. Here, labels contains the annotated cell IDs for the segmented neurons in our image. Please note in the description of the volume segmentation what the weight and \n",
    "labels represent.\n",
    "\n",
    "In the example files we use here, we are only segmenting for the center of neurons and not regions associated with each neuron. The volume segmentation works the same regardless of if \n",
    "you have single points or full masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_file =  datapath + 'blobs.csv'\n",
    "blobs = pd.read_csv(blob_file)\n",
    "\n",
    "IDs = blobs['ID']\n",
    "labels = IDs.replace(np.nan,'',regex=True)\n",
    "labels = list(np.asarray(labels)) \n",
    "#Need list of ID labels to add as extra column to PlaneSegmentation\n",
    "\n",
    "vs = PlaneSegmentation(\n",
    "    name = 'NeuroPALNeurons',\n",
    "    description = 'Neuron centers for multichannel volumetric image. Weight set at 1 for all voxels. Labels refers to cell ID of segmented neurons',\n",
    "    #Reference the same ImagingVolume that your image was taken with\n",
    "    imaging_plane = ImagingVol,\n",
    ")\n",
    "\n",
    "#Uncomment the code below to use image_mask instead of voxel_mask\n",
    "'''\n",
    "for i, row in blobs.iterrows():\n",
    "    #image_mask should have the same size as the x, y, z dimensions of the raw image\n",
    "    image_mask = np.zeros(data.shape[:-1])\n",
    "    x = row['X']\n",
    "    y = row['Y']\n",
    "    z = row['Z']\n",
    "    ID = row['ID']\n",
    "\n",
    "    #we define the ROI here as a 3x3x3 region surrounding the neuron center\n",
    "    image_mask[x-1:x+1, y-1:y+1, z-1:z+1] = 1\n",
    "\n",
    "'''\n",
    "\n",
    "#Uncomment the code below to use voxel_mask instead of image_mask\n",
    "for i, row in blobs.iterrows():\n",
    "    voxel_mask = []\n",
    "    x = row['X']\n",
    "    y = row['Y']\n",
    "    z = row['Z']\n",
    "\n",
    "    voxel_mask.append([np.uint(x),np.uint(y),np.uint(z),1]) #If using a segmentation mask rather than neuron center, voxel_mask should be a list of all voxels associated with the ROI\n",
    "\n",
    "    vs.add_roi(voxel_mask=voxel_mask)\n",
    "\n",
    "vs.add_column(\n",
    "     name = 'ID_labels',\n",
    "     description = 'ROI ID labels',\n",
    "     data = labels,\n",
    "     index=True, \n",
    ")\n",
    "\n",
    "NeuroPALImSeg = ImageSegmentation(\n",
    "    name = 'NeuroPALSegmentation',\n",
    ")\n",
    "\n",
    "NeuroPALImSeg.add_plane_segmentation(vs)\n",
    "\n",
    "NeuroPALImSeg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding calcium imaging data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the tifffile package to read in our calcium imaging tif files and examine the metadata contained in the file.\n",
    "We see that there are 18000 pages, each with shape (240, 1000) which are associated with the YX dimensions. For our data,\n",
    "each page represents a Z slice at a specific point in time. There are 12 Z slices per volume and 1500 time points\n",
    "(1500*12=18000). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import TiffFile\n",
    "\n",
    "tif = TiffFile(datapath+'Calc_series_raw.tiff')\n",
    "\n",
    "print(len(tif.pages))\n",
    "\n",
    "page = tif.pages[0]\n",
    "\n",
    "print(page.shape)\n",
    "print(page.dtype)\n",
    "print(page.axes)\n",
    "\n",
    "series = tif.series[0]\n",
    "print(series.shape)\n",
    "print(series.axes)\n",
    "\n",
    "print(len(tif.pages))\n",
    "\n",
    "tif.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your calcium imaging data is relatively small (less than a few gigabytes) you may be able to just open your raw files and add the corresponding\n",
    "data array to an NWB object in just the same way that we did for the NeuroPAL structural images. In practice, most calcium imaging data will be \n",
    "much larger than that and likely cannot be loaded all at once into memory. To solve this, we will have to do what's called an iterative data write.\n",
    "\n",
    "In an iterative data write, we define a method to iteratively load portions of the larger data array and stitch them together in the NWB file. Once\n",
    "we define the method for generating the data, NWB will handle the rest. See https://pynwb.readthedocs.io/en/stable/tutorials/advanced_io/plot_iterative_write.html#sphx-glr-tutorials-advanced-io-plot-iterative-write-py for other examples and information of iterative data writes.\n",
    "\n",
    "We will use the MultiChannelVolumeSeries object to store our raw calcium images. This object takes in objects of dimensions (T, X, Y, Z, C), representing time points (frames), XYZ image lengths, and number of channels. \n",
    "\n",
    "Some recent datasets, especially in freely moving worms, use an additional red reference channel to account for motion artifacts that may be impact the GCaMP signal. Our datasets do not use this reference channel so we have just one channel in our MultiChannelVolumeSeries. Adding additional channels should be easy and can be done in the same way that we do above for our MultiChannelVolumes. Simply add the extra channels to the CalcChannels list, and in your DataChunkIterator function make sure to yield all channels you want included in the image. Make sure to add a comment to the MultiChannelVolume object stsating what information each channel holds. Ex. 'GFP channel contains GCaMP calcium signal, mNeptune contains reference'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a data generator function that will yield a single data entry, in our case we are iterating over time points and creating a Z stack of images for each time point\n",
    "def iter_calc_tiff(filename, numZ):\n",
    "\n",
    "    #TiffFile object allows you to access metadata for the tif file and selectively load individual pages/series\n",
    "    tif = TiffFile(filename)\n",
    "\n",
    "    #In this dataset, one page is one XY plane and every 12 pages comprises one Z stack for an individual time point\n",
    "    pages = len(tif.pages)\n",
    "    timepoints = int(pages/numZ)\n",
    "\n",
    "    pageshape = tif.pages[0].shape\n",
    "\n",
    "    #We iterate through all of the timepoints and yield each timepoint back to the DataChunkIterator\n",
    "    for i in range(timepoints):\n",
    "        tpoint = np.zeros((pageshape[1],pageshape[0], numZ))\n",
    "        for j in range(numZ):\n",
    "            image = np.transpose(tif.pages[i*numZ+j].asarray())\n",
    "            tpoint[:,:,j] = image\n",
    "\n",
    "        #Make sure array ends up as the correct dtype coming out of this function (the dtype that your data was collected as)\n",
    "        yield tpoint.astype('uint16')\n",
    "\n",
    "    tif.close()\n",
    "\n",
    "    return\n",
    "\n",
    "#The DataChunkIterator wraps the data generator function and will stitch together the chunks as it iteratively reads over the full file\n",
    "data = DataChunkIterator(\n",
    "    data= iter_calc_tiff(datapath+'Calc_series_raw.tiff', 12),\n",
    "    #this will be the max shape of the final image. Can leave blank or set as the size of your full data if you know that ahead of time\n",
    "    maxshape = None,\n",
    "    buffer_size = 10,\n",
    ")\n",
    "\n",
    "CalcChannels = [(\"GFP-GCaMP\", \"Chroma ET 525/50\",\"488-525-50m\")]\n",
    "\n",
    "CalcOptChannels = []\n",
    "CalcOptChanRefData = []\n",
    "#The loop below takes the list of channels and converts it into a list of OpticalChannelPlus objects which hold the metadata\n",
    "#for the optical channels used in the experiment\n",
    "for fluor, des, wave in CalcChannels:\n",
    "    excite = float(wave.split('-')[0])\n",
    "    emiss_mid = float(wave.split('-')[1])\n",
    "    emiss_range = float(wave.split('-')[2][:-1])\n",
    "    OptChan = OpticalChannelPlus(\n",
    "        name = fluor,\n",
    "        description = des,\n",
    "        excitation_lambda = excite,\n",
    "        excitation_range = [excite-1.5, excite+1.5],\n",
    "        emission_range = [emiss_mid-emiss_range/2, emiss_mid+emiss_range/2],\n",
    "        emission_lambda = emiss_mid\n",
    "    )\n",
    "\n",
    "    CalcOptChannels.append(OptChan)\n",
    "    CalcOptChanRefData.append(wave)\n",
    "\n",
    "\n",
    "#This object just contains references to the order of channels because OptChannels does not preserve ordering\n",
    "CalcOptChanRefs = OpticalChannelReferences(\n",
    "    name = 'OpticalChannelRefs',\n",
    "    channels = CalcOptChanRefData\n",
    ")\n",
    "\n",
    "CalcImagingVolume = ImagingVolume(\n",
    "    name = 'CalciumImVol',\n",
    "    description = 'Imaging volume used to acquire calcium imaging data',\n",
    "    optical_channel_plus = CalcOptChannels,\n",
    "    order_optical_channels = CalcOptChanRefs,\n",
    "    device = device,\n",
    "    location = 'Worm head',\n",
    "    grid_spacing = [0.3208, 0.3208, 2.5],\n",
    "    grid_spacing_unit = 'um',\n",
    "    reference_frame = 'Worm head'\n",
    ")\n",
    "\n",
    "wrapped_data = H5DataIO(data=data, compression=\"gzip\", compression_opts=4)\n",
    "\n",
    "calcium_image_series = MultiChannelVolumeSeries(\n",
    "    name=\"CalciumImageSeries\",\n",
    "    description = \"Raw GCaMP series images\",\n",
    "    comments = \"Include information about what each channel represents here: e.g. GFP-GCaMP channel represents GCaMP signal, mNeptune represents reference signal\",\n",
    "    data=wrapped_data,\n",
    "    device = device,\n",
    "    unit=\"Voxel gray counts\", \n",
    "    scan_line_rate = 2995.,\n",
    "    dimension = [1000,240,12],\n",
    "    resolution = 1., #smallest meaningful difference (in specified unit) between values in data: i.e. level of precision\n",
    "    rate = 1.04, #sampling rate in hz\n",
    "    imaging_volume = CalcImagingVolume,\n",
    ")\n",
    "\n",
    "\n",
    "nwbfile.add_imaging_plane(CalcImagingVolume)\n",
    "\n",
    "calcium_image_series"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have added our calcium imaging raw data to the NWB file, we can add our processed calcium imaging data which consists\n",
    "of ROI locations and fluorescence for each time point. \n",
    "\n",
    "To do this, we will be using the ImageSegmentation, PlaneSegmentation, Fluorescence, and ROIResponseSeries objects native to the Pynwb\n",
    "OptoPhysiology spec.\n",
    "\n",
    "Our calcium quantification data is stored in a CSV 'gce_quant.csv' file with columns [Z, Y, X, T, blob_ix, ID, gce_quant]. This is a long dataframe where each row in this CSV file contains the position, cell ID (currently blank in this dataset), and quantification information for each blob at each time point. Every blob_ix should exist at every time point. If a blob is missing from a time point, please add an empty row with just blob_ix filled in. We will add those NaN rows to the volume segmentation and set the weight to 0 to signify missing data.\n",
    "\n",
    "We initially will convert that CSV file into a 3D numpy array with blob_ix on one dimension, time on another dimension, and X, Y, Z, ID, gce_quant on the final dimension.\n",
    "\n",
    "We will then create plane segmentations for each time point that contain the locations for each neuron at that time point and load them into\n",
    "the same ImageSegmentation object. \n",
    "\n",
    "We then create an roi_table_region to add the fluorescence data to which indicates which ROIs the fluorescence data applies to (in this case all\n",
    "of them). The RoiResponseSeries is then wrapped by the DfoF object to indicate that the RoiResponseSeries refers to DfoF data.\n",
    "There is also a fluorescence object native to PyNWB which does the same thing for raw fluorescence data (i.e. voxel gray counts).\n",
    "\n",
    "Here I will define three fluorescence objects. One for the green signal channel, one for the red reference channel, and one for the processed data created by correcting the signal channel via the reference channel. Again, we only have the green signal channel in our own data so we will only include that fluorescence signal here, but I am leaving space for the other fluorescence objects commented out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gce_quant = pd.read_csv(datapath+'gce_quant.csv')\n",
    "\n",
    "print(len(gce_quant['blob_ix'].unique())) #Count the number of unique blobs in this file\n",
    "print(len(gce_quant['T'].unique())) #Count the number of unique time points in this file\n",
    "\n",
    "quant = gce_quant[['X', 'Y', 'Z', 'gce_quant', 'ID', 'T', 'blob_ix']] #Reorder columns to order we want\n",
    "\n",
    "blobquant = None\n",
    "for idx in gce_quant['blob_ix'].unique():\n",
    "    blob = quant[quant['blob_ix']==idx]\n",
    "    blobarr = np.asarray(blob[['X','Y','Z','gce_quant','ID']]) \n",
    "    blobarr = blobarr[np.newaxis, :, :]\n",
    "    if blobquant is None:\n",
    "        blobquant=blobarr\n",
    "\n",
    "    else:\n",
    "        blobquant = np.vstack((blobquant, blobarr))\n",
    "\n",
    "print(blobquant.shape) #Now dimensions are blob_ix, time, and data columns (X, Y, Z, gce_quant, ID). We are now ready to add this data to NWB objects.\n",
    "\n",
    "volsegs = []\n",
    "\n",
    "for t in range(blobquant.shape[1]):\n",
    "    volseg = PlaneSegmentation(\n",
    "        name = 'Seg_tpoint_'+str(t),\n",
    "        description = 'Neuron segmentation for time point ' +str(t) + ' in calcium image series',\n",
    "        imaging_plane = CalcImagingVolume,\n",
    "        reference_images = calcium_image_series,\n",
    "    )\n",
    "\n",
    "    for i in range(blobquant.shape[0]):\n",
    "        voxel_mask = blobquant[i,t,0:3] #X, Y, Z columns\n",
    "        if np.any(np.isnan(voxel_mask)):\n",
    "            voxel_mask = np.asarray([0,0,0,0]) #if blob does not exist at time point (nan values in row) we replace values with 0 and set weight to 0\n",
    "        else:\n",
    "            voxel_mask = np.hstack((voxel_mask, 1)) #add weight of one to each blob\n",
    "        voxel_mask = voxel_mask[np.newaxis,:] #add empty new axis to make shape compatible\n",
    "\n",
    "        volseg.add_roi(voxel_mask=voxel_mask)\n",
    "\n",
    "    volsegs.append(volseg)\n",
    "\n",
    "ImSeg = ImageSegmentation(\n",
    "    name = 'CalciumSeriesSegmentation', #use if tracking neurons across frames (correspondence between segmentations)\n",
    "    #name = 'CalciumSeriesSegmentationUntracked' #use if not trakcing across frames (ie raw segmentation in each frame)\n",
    "    plane_segmentations = volsegs\n",
    ")\n",
    "\n",
    "gce_data = np.transpose(blobquant[:,:,3]) #Take only gce quantification column and transpose so time is in the first dimension\n",
    "\n",
    "rt_region = volsegs[0].create_roi_table_region(\n",
    "    description = 'All segmented neurons associated with calcium image series',\n",
    "    region = list(np.arange(blobquant.shape[0]))\n",
    ")\n",
    "\n",
    "#If you have raw fluorescence values rather than DFoF use the Fluorescence object instead of the DfOverF object to save your RoiResponseSeries\n",
    "SignalRoiResponse = RoiResponseSeries(\n",
    "    #See https://pynwb.readthedocs.io/en/stable/pynwb.ophys.html#pynwb.ophys.RoiResponseSeries for additional key word argument options\n",
    "    name = 'SignalCalciumImResponseSeries',\n",
    "    description = 'DF/F activity for calcium imaging data',\n",
    "    data = gce_data, #first dimension should represent time and second dimension should represent ROIs\n",
    "    rois = rt_region,\n",
    "    unit = 'Percentage', #the unit of measurement for the data input here\n",
    "    resolution = 0.01,\n",
    "    rate = 1.04\n",
    ")\n",
    "\n",
    "SignalFluor = DfOverF( #Change to Fluorescence if using raw fluorescence\n",
    "    name = 'SignalDFoF', #Change name to SignalRawFluor if using raw fluorescence, rename reference and processed object accordingly\n",
    "    roi_response_series = SignalRoiResponse\n",
    ")\n",
    "\n",
    "'''\n",
    "\n",
    "RefRoiResponse = RefRoiResponseSeries(\n",
    "    #See https://pynwb.readthedocs.io/en/stable/pynwb.ophys.html#pynwb.ophys.RoiResponseSeries for additional key word argument options\n",
    "    name = 'ReferenceCalciumImResponseSeries',\n",
    "    description = 'Fluorescence for reference channel in calcium imaging',\n",
    "    data = ref_data, #first dimension should represent time and second dimension should represent ROIs\n",
    "    rois = rt_region, \n",
    "    unit = '', #the unit of measurement for the data input here\n",
    "    rate = 4.0\n",
    ")\n",
    "\n",
    "RefFluor = DfOverF(\n",
    "    name = 'ReferenceDFoF',\n",
    "    roi_response_series = RefRoiResponse\n",
    ")\n",
    "\n",
    "ProcRoiResponse = RoiResponseSeries(\n",
    "    #See https://pynwb.readthedocs.io/en/stable/pynwb.ophys.html#pynwb.ophys.RoiResponseSeries for additional key word argument options\n",
    "    name = 'ProcessedCalciumImResponseSeries',\n",
    "    description = 'processed calcium fluorescence activity - describe processing steps taken to obtain',\n",
    "    data = proc_gce_data, #first dimension should represent time and second dimension should represent ROIs\n",
    "    rois = rt_region,\n",
    "    unit = '', #the unit of measurement for the data input here\n",
    "    rate = 4.0\n",
    ")\n",
    "\n",
    "ProcFluor = DfoF(\n",
    "    name = 'ProcessedDFoF',\n",
    "    roi_response_series = RoiResponse\n",
    ")\n",
    "\n",
    "'''\n",
    "\n",
    "ImSeg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the NWB file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created all the objects that we need to add to the NWB file, the next step is to actually add them to the file and then write the file to a specific location.\n",
    "\n",
    "Objects are either put in the base acquisition module or in processing modules that users define. Only raw, acquired data should be put into the acquisition module, everything \n",
    "else should be put in processing modules. You can define any number of processing modules and add them to the base NWB file, but generally speaking each type of data that you\n",
    "are adding to the file should have its own processing module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we add our raw NeuroPAL image to the acquisition module of the base NWB file\n",
    "nwbfile.add_acquisition(Image)\n",
    "nwbfile.add_acquisition(calcium_image_series)\n",
    "\n",
    "#we create a processing module for our neuroPAL data \n",
    "neuroPAL_module = nwbfile.create_processing_module(\n",
    "    name = 'NeuroPAL',\n",
    "    description = 'NeuroPAL image metadata and segmentation'\n",
    ")\n",
    "\n",
    "neuroPAL_module.add(NeuroPALImSeg)\n",
    "neuroPAL_module.add(OpticalChannelRefs)\n",
    "\n",
    "#we create a processing module for the pre-processed neuroPAL image\n",
    "processed_im_module = nwbfile.create_processing_module(\n",
    "    name = 'ProcessedImage',\n",
    "    description = 'Data and metadata associated with the processed neuroPAL image'\n",
    ")\n",
    "\n",
    "processed_im_module.add(ProcImage)\n",
    "processed_im_module.add(ProcOpticalChannelRefs)\n",
    "\n",
    "ophys = nwbfile.create_processing_module(\n",
    "    name = 'CalciumActivity',\n",
    "    description = 'Calcium time series metadata, segmentation, and fluorescence data'\n",
    ")\n",
    "\n",
    "ophys.add(ImSeg)\n",
    "ophys.add(SignalFluor)\n",
    "ophys.add(CalcOptChanRefs)\n",
    "#ophys.add(RefFluor)\n",
    "#ophys.add(ProcFluor)\n",
    "\n",
    "\n",
    "#each NWB file should be named with a unique identifier \n",
    "identifier = '20230322-21-41-10'\n",
    "\n",
    "#specify the file path you want to save this NWB file to \n",
    "io = NWBHDF5IO(datapath+'identifier.nwb', mode='w')\n",
    "io.write(nwbfile)\n",
    "io.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from existing NWB files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You read NWB files using the same NWBHDF5IO package that was used to write files. \n",
    "\n",
    "You can print the entire nwb file to see the contents of the file, including all modules and \n",
    "objects present. Each object will be shown in the field as the 'name' field that you assigned \n",
    "to that object when creating it. \n",
    "\n",
    "NWBHDF5IO does what is called a lazy read. This means when you load the object, it does not \n",
    "actually load the whole object into memory. You have to specify specific slices of the data to load.\n",
    "\n",
    "In the example below we add [:] at the end of the variable declaration to indicate that we want to load\n",
    "the entirety of that object into the variable. We could also select specific slices of that data.\n",
    "For example, if we wanted to only load the first 4 channels of one Z slice of the raw neuroPAL data we\n",
    "could do this by calling read_nwbfile.acquisition['NeuroPALImageRaw].data[:,:,7,0:4]. This indexing \n",
    "means we take all of the first two dimensions, the 7th slice of the third dimension (python indexing starts from 0),\n",
    "and the first 4 slices of the last dimension (the channels). \n",
    "\n",
    "This lazy read is especially helpful when working with very large data, like calcium imaging data, because\n",
    "it allows us to iteratively read in segments of that large data instead of having to load it into memory\n",
    "all at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with NWBHDF5IO(datapath+'example.nwb', mode='r', load_namespaces=True) as io:\n",
    "    read_nwbfile = io.read()\n",
    "    print(read_nwbfile) # you can print the nwbfile after reading to see the contents of the file\n",
    "    print(read_nwbfile.processing['CalciumActivity'])\n",
    "    print(read_nwbfile.processing['NeuroPAL'])\n",
    "    print(read_nwbfile.processing['ProcessedImage'])\n",
    "    subject = read_nwbfile.subject #get the metadata about the experiment subject\n",
    "    growth_stage = subject.growth_stage\n",
    "    image = read_nwbfile.acquisition['NeuroPALImageRaw'].data[:] #get the neuroPAL image as a np array\n",
    "    channels = read_nwbfile.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "    im_vol = read_nwbfile.acquisition['NeuroPALImageRaw'].imaging_volume #get the metadata associated with the imaging acquisition\n",
    "    seg = read_nwbfile.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:] #get the locations of neuron centers\n",
    "    labels = read_nwbfile.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "    optchans = im_vol.optical_channel_plus[:] #get information about all of the optical channels used in acquisition\n",
    "    chan_refs = read_nwbfile.processing['NeuroPAL']['OpticalChannelRefs'].channels[:] #get the order of the optical channels in the image\n",
    "    proc_image = read_nwbfile.processing['ProcessedImage']['ProcessedImage'].data[:] #get the pre-processed image\n",
    "    calcium_frames = read_nwbfile.acquisition['CalciumImageSeries'].data[0:15, :,:,:] #load the first 15 frames of the calcium images\n",
    "    fluor = read_nwbfile.processing['CalciumActivity']['SignalDFoF']['SignalCalciumImResponseSeries'].data[:]\n",
    "    #calc_seg = read_nwbfile.processing['CalciumActivity']['CalciumSeriesSegmentation']['Seg_tpoint_0'].voxel_mask[:]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've read in our objects from our NWB file, we can run analyses and create visualizations with\n",
    "the data we have. Here I will show an example of displaying XZ and XY projections of the processed neuroPAL \n",
    "image and overlaying the segmented neuron centers on top.\n",
    "\n",
    "Ideally, you could embed NWB into your full data processing pipeline, saving all of your raw and intermediary\n",
    "data to the NWB file and reading objects directly from it as well. This eliminates the need to interface with \n",
    "almost any additional file types in your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"\".join(label) for label in labels]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = pd.DataFrame.from_records(seg, columns = ['X', 'Y', 'Z', 'weight', 'ID'])\n",
    "blobs = blobs.drop(['weight'], axis=1)\n",
    "blobs = blobs.replace('nan', np.nan, regex=True) \n",
    "\n",
    "print(proc_image.shape)\n",
    "\n",
    "RGB = proc_image[:,:,:,:-1]\n",
    "\n",
    "print(RGB.shape)\n",
    "\n",
    "Zmax = np.max(RGB, axis=2)\n",
    "Ymax = np.max(RGB, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(np.transpose(Zmax, [1,0,2]))\n",
    "plt.scatter(blobs['x'], blobs['y'], s=5)\n",
    "plt.xlim((0, Zmax.shape[0]))\n",
    "plt.ylim((0, Zmax.shape[1]))\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(np.transpose(Ymax, [1,0,2]))\n",
    "plt.scatter(blobs['x'], blobs['z'], s=5)\n",
    "plt.xlim((0, Ymax.shape[0]))\n",
    "plt.ylim((0, Ymax.shape[1]))\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing existing NWB file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can flexibly add or remove objects from NWB files without having to completely rewrite the whole file.\n",
    "See https://pynwb.readthedocs.io/en/stable/tutorials/general/add_remove_containers.html#sphx-glr-tutorials-general-add-remove-containers-py\n",
    "for a tutorial on adding and removing objects.\n",
    "\n",
    "Adding new objects is fairly straightforward. You just need to open the file, add new objects you create, and then rewrite the file todisk.\n",
    "\n",
    "Removing objects is also fairly straightforward, but be careful you do not accidentally delete objects that are referenced by other objects\n",
    "in the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement example of editing existing NWB file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NWB-dev",
   "language": "python",
   "name": "nwb-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
