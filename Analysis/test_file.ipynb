{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "from hdmf.backends.hdf5.h5_utils import H5DataIO\n",
    "from hdmf.container import Container\n",
    "from hdmf.data_utils import DataChunkIterator\n",
    "import latex\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pynwb import load_namespaces, get_class, register_class, NWBFile, TimeSeries, NWBHDF5IO\n",
    "from pynwb.file import MultiContainerInterface, NWBContainer, Device, Subject\n",
    "from pynwb.ophys import ImageSeries, OnePhotonSeries, OpticalChannel, ImageSegmentation, PlaneSegmentation, Fluorescence, DfOverF, CorrectedImageStack, MotionCorrection, RoiResponseSeries, ImagingPlane\n",
    "from pynwb.core import NWBDataInterface\n",
    "from pynwb.epoch import TimeIntervals\n",
    "from pynwb.behavior import SpatialSeries, Position\n",
    "from pynwb.image import ImageSeries\n",
    "import pywt\n",
    "import scipy.io as sio\n",
    "import scipy\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import seaborn as sns\n",
    "import skimage.io as skio\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from tifffile import TiffFile\n",
    "import tifffile\n",
    "\n",
    "from atlas import loadmat, NPAtlas, NWBAtlas\n",
    "from process_file import get_nwb_neurons, get_dataset_neurons, get_dataset_online, combine_datasets, get_pairings, get_color_discrim, get_neur_nums\n",
    "from stats import get_summary_stats, analyze_pairs, get_accuracy\n",
    "from visualization import plot_num_heatmap, plot_std_heatmap, plot_summary_stats, plot_color_discrim, plot_accuracies, plot_visualizations_atlas, plot_visualizations_data, plot_atlas2d_super\n",
    "from utils import covar_to_coord, convert_coordinates, maha_dist, run_linear_assignment\n",
    "\n",
    "# ndx_mulitchannel_volume is the novel NWB extension for multichannel optophysiology in C. elegans\n",
    "from ndx_multichannel_volume import CElegansSubject, OpticalChannelReferences, OpticalChannelPlus, ImagingVolume, VolumeSegmentation, MultiChannelVolume, MultiChannelVolumeSeries\n",
    "\n",
    "import os\n",
    "import PyQt6.QtCore\n",
    "os.environ[\"QT_API\"] = \"pyqt6\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyQt6.QtCore\n",
    "os.environ[\"QT_API\"] = \"pyqt6\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/danielysprague/foco_lab/data/NP_nwb/56_YAaDV.nwb'\n",
    "#filepath = '/Users/danielysprague/foco_lab/data/NWB_foco/2021-12-03-w00-NP1.nwb'\n",
    "\n",
    "with NWBHDF5IO(filepath, mode='r', load_namespaces=True) as io:\n",
    "    read_nwbfile = io.read()\n",
    "    #print(read_nwbfile.processing['ProcessedImage'])\n",
    "    subject = read_nwbfile.subject #get the metadata about the experiment subject\n",
    "    growth_stage = subject.growth_stage\n",
    "    image = read_nwbfile.acquisition['NeuroPALImageRaw'].data[:] #get the neuroPAL image as a np array\n",
    "    channels = read_nwbfile.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "    im_vol = read_nwbfile.acquisition['NeuroPALImageRaw'].imaging_volume #get the metadata associated with the imaging acquisition\n",
    "    seg = read_nwbfile.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:] #get the locations of neuron centers\n",
    "    labels = read_nwbfile.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "    optchans = im_vol.optical_channel_plus[:] #get information about all of the optical channels used in acquisition\n",
    "    chan_refs = read_nwbfile.processing['NeuroPAL']['OpticalChannelRefs'].channels[:] #get the order of the optical channels in the image\n",
    "    #calcium_frames = read_nwbfile.acquisition['CalciumImageSeries'].data[0:15, :,:,:] #load the first 15 frames of the calcium images\n",
    "    #print(read_nwbfile.acquisition['CalciumImageSeries'].dimension[:])\n",
    "    #fluor = read_nwbfile.processing['CalciumActivity']['SignalRawFluor']['SignalCalciumImResponseSeries'].data[:]\n",
    "    #calc_labels = read_nwbfile.processing['CalciumActivity']['NeuronIDs'].labels[:]\n",
    "\n",
    "    NIR = read_nwbfile.processing['BF_NIR']['BrightFieldNIR'].data[:,:,:]\n",
    "\n",
    "read_nwbfile\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = fluor.T\n",
    "\n",
    "ref_ind = np.argwhere(calc_labels=='AVAR')\n",
    "ref_trace = np.squeeze(ts[ref_ind,:])\n",
    "\n",
    "to_plot = ref_trace / np.mean(ref_trace)\n",
    "\n",
    "print(ref_ind)\n",
    "print(ref_trace.shape)\n",
    "\n",
    "plt.plot(ref_trace)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_max = np.max(calcium_frames, axis=0)\n",
    "zmax_calc = np.max(calcium_frames[10,:,:,:], axis=2)\n",
    "plt.imshow(zmax_calc)\n",
    "plt.show()\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(NIR[10,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = pd.DataFrame.from_records(seg, columns = ['X', 'Y', 'Z', 'weight', 'ID'])\n",
    "blobs = blobs.drop(['weight'], axis=1)\n",
    "blobs = blobs.replace('nan', np.nan, regex=True) \n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "RGB = image[:,:,:,channels[:-1]]/np.max(image)\n",
    "\n",
    "print(RGB.shape)\n",
    "\n",
    "Zmax = np.max(RGB, axis=2)\n",
    "Ymax = np.max(RGB, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(np.transpose(Zmax, [1,0,2]))\n",
    "plt.scatter(blobs['x'], blobs['y'], s=5)\n",
    "plt.xlim((0, Zmax.shape[0]))\n",
    "plt.ylim((0, Zmax.shape[1]))\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.imshow(np.transpose(Ymax, [1,0,2]))\n",
    "plt.scatter(blobs['x'], blobs['z'], s=5)\n",
    "plt.xlim((0, Ymax.shape[0]))\n",
    "plt.ylim((0, Ymax.shape[1]))\n",
    "plt.gca().set_aspect('equal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib qt\n",
    "\n",
    "#filepath = '/Users/danielysprague/foco_lab/data/NWB_Ray/20230506-15-01-45.nwb'\n",
    "#filepath = '/Users/danielysprague/foco_lab/data/NWB_foco/2021-12-03-w00-NP1.nwb'\n",
    "filepath = '/Users/danielysprague/foco_lab/data/Yemini_NWB/20190924_01.nwb'\n",
    "#filepath = '/Users/danielysprague/foco_lab/data/kimura_full/sub-230928-02_ses-20230928T111400_ophys.nwb'\n",
    "\n",
    "with NWBHDF5IO(filepath, mode='r', load_namespaces=True) as io:\n",
    "    read_nwbfile = io.read()\n",
    "    #print(read_nwbfile.processing['ProcessedImage'])\n",
    "    subject = read_nwbfile.subject #get the metadata about the experiment subject\n",
    "    growth_stage = subject.growth_stage\n",
    "    channels = read_nwbfile.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "    im_vol = read_nwbfile.acquisition['NeuroPALImageRaw'].imaging_volume #get the metadata associated with the imaging acquisition\n",
    "    seg = read_nwbfile.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:] #get the locations of neuron centers\n",
    "    labels = read_nwbfile.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "    optchans = im_vol.optical_channel_plus[:] #get information about all of the optical channels used in acquisition\n",
    "    chan_refs = read_nwbfile.processing['NeuroPAL']['OpticalChannelRefs'].channels[:] #get the order of the optical channels in the image\n",
    "    rate = read_nwbfile.acquisition['CalciumImageSeries'].rate\n",
    "    fluor = read_nwbfile.processing['CalciumActivity']['SignalRawFluor']['SignalCalciumImResponseSeries'].data[:]\n",
    "    calc_labels = read_nwbfile.processing['CalciumActivity']['NeuronIDs'].labels[:]\n",
    "\n",
    "\n",
    "traces = np.transpose(fluor)\n",
    "\n",
    "labels = [\"\".join(label) for label in labels]\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "\n",
    "def plot_traces(traces, rate, labels, selected):\n",
    "\n",
    "    plt.rcParams.update({'font.size':20})\n",
    "\n",
    "    seconds = traces.shape[1]//rate\n",
    "\n",
    "    fig, axs = plt.subplots(len(selected),1, figsize=(5,6))\n",
    "\n",
    "    for i, neuron in enumerate(selected):\n",
    "        index = np.argwhere(np.asarray(labels)==neuron)\n",
    "        trace = traces[np.squeeze(index),:]\n",
    "\n",
    "        axs[i].plot(np.linspace(0,seconds,traces.shape[1]), trace)\n",
    "        axs[i].set_ylabel(r'$\\Delta$F/F')\n",
    "        axs[i].set_xlim(0,seconds)\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_title(neuron, loc='left')\n",
    "\n",
    "        axs[i].spines['right'].set_visible(False)\n",
    "        axs[i].spines['top'].set_visible(False)\n",
    "\n",
    "    axs[4].set_xlabel('time (seconds)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_traces(traces, rate, labels, ['AVAR', 'SMDVR', 'AWCR','RID', 'ASHR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import tvregdiff\n",
    "\n",
    "def run_pca(traces, labels, reference, rate, n_components):\n",
    "    deriv_iter = 5\n",
    "    deriv_alpha = 0.001\n",
    "    derivs = np.zeros(traces.shape)\n",
    "    for n in range(traces.shape[0]):\n",
    "        trace = traces[n,:]\n",
    "        deriv = tvregdiff.TVRegDiff(trace, deriv_iter, deriv_alpha, cgmaxit=100, diagflag=False, plotflag=False)\n",
    "\n",
    "        derivs[n,:] = deriv\n",
    "        \n",
    "    X = derivs.T\n",
    "    keep_indices = ~np.isnan(X).any(axis=0)\n",
    "    X = X[:,keep_indices]\n",
    "    keep_labels = labels[keep_indices]\n",
    "\n",
    "    X = scipy.signal.detrend(X, axis=0)\n",
    "\n",
    "    pca = PCA(n_components = n_components)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    pipe = Pipeline(steps= [(\"scaler\", scaler), (\"pca\", pca)])\n",
    "    pipe.fit(X)\n",
    "\n",
    "    mat = derivs.T\n",
    "    mat = mat[:, keep_indices]\n",
    "    transform = pipe.transform(mat)\n",
    "\n",
    "    ref_ind = np.argwhere(labels==reference)\n",
    "    ref_trace = derivs[ref_ind,:]\n",
    "\n",
    "    weights = pca.components_.T\n",
    "\n",
    "    x1 = np.squeeze(ref_trace)\n",
    "    x2 = transform[:,0]\n",
    "\n",
    "    corr = scipy.signal.correlate(x1,x2)\n",
    "    lags = scipy.signal.correlation_lags(len(x1), len(x2))\n",
    "\n",
    "    xvals = lags/rate\n",
    "    window = 40\n",
    "\n",
    "    # calculate xcorr between pc1 and reference, flip if xcorr<0\n",
    "    bounds = [len(lags) // 2 - window, len(lags) // 2 + window]\n",
    "    to_plot_x = xvals[bounds[0] : bounds[1]]\n",
    "    to_plot_y = corr[bounds[0] : bounds[1]]\n",
    "    peak_xcorr_value = to_plot_y[np.argmax(np.abs(to_plot_y))]\n",
    "    if peak_xcorr_value < 0:\n",
    "        transform[:, 0] = -1 * transform[:, 0]\n",
    "        weights[:, 0] = -1 * weights[:, 0]\n",
    "\n",
    "    if n_components > 1:\n",
    "        x2 = transform[:,1] #pc2\n",
    "        corr = scipy.signal.correlate(x1,x2)\n",
    "        lags = scipy.signal.correlation_lags(len(x1), len(x2))\n",
    "\n",
    "        xvals = lags / rate\n",
    "\n",
    "        # calculate xcorr between pc2 and ava\n",
    "        xvals = lags / rate\n",
    "        bounds = [len(lags) // 2 - window, len(lags) // 2 + window]\n",
    "        to_plot_x = xvals[bounds[0] : bounds[1]]\n",
    "        to_plot_y = corr[bounds[0] : bounds[1]]\n",
    "        peak_xcorr = to_plot_x[np.argmax(to_plot_y)]\n",
    "\n",
    "        # if peak xcorr > 0, flip pc2\n",
    "        if peak_xcorr > -1:\n",
    "            transform[:, 1] = -1 * transform[:, 1]\n",
    "            weights[:, 1] = -1 * weights[:, 1]\n",
    "\n",
    "        x2 = transform[:, 2] #pc3\n",
    "        corr = scipy.signal.correlate(x1, x2)\n",
    "        lags = scipy.signal.correlation_lags(len(x1), len(x2))\n",
    "        xvals = lags / rate\n",
    "        bounds = [len(lags) // 2 - window, len(lags) // 2 + window]\n",
    "        to_plot_x = xvals[bounds[0] : bounds[1]]\n",
    "        to_plot_y = corr[bounds[0] : bounds[1]]\n",
    "        xcorr_at_0 = to_plot_y[window]\n",
    "        if xcorr_at_0 < 0:\n",
    "            transform[:, 2] = -1 * transform[:, 2]\n",
    "            weights[:, 2] = -1 * weights[:, 2]\n",
    "\n",
    "    return pca, weights, transform, keep_labels\n",
    "'''  \n",
    "filepath = '/Users/danielysprague/foco_lab/data/NWB_Ray/20230506-15-01-45.nwb'\n",
    "#filepath = '/Users/danielysprague/foco_lab/data/Yemini_nwb/20190924_01.nwb'\n",
    "#filepath = '/Users/danielysprague/foco_lab/data/NWB_foco/2021-12-03-w00-NP1.nwb'\n",
    "\n",
    "with NWBHDF5IO(filepath, mode='r', load_namespaces=True) as io:\n",
    "    read_nwb = io.read()\n",
    "    identifier = read_nwb.identifier\n",
    "    seg = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:]\n",
    "    labels = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "    #labels_index = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels_index'][:]\n",
    "    channels = read_nwb.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "    image = read_nwb.acquisition['NeuroPALImageRaw'].data[:]\n",
    "    scale = read_nwb.imaging_planes['NeuroPALImVol'].grid_spacing[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "\n",
    "    rate = read_nwb.acquisition['CalciumImageSeries'].rate\n",
    "    #fluor = read_nwb.processing['CalciumActivity']['SignalRawFluor']['SignalCalciumImResponseSeries'].data[:]\n",
    "    dfof = read_nwb.processing['CalciumActivity']['SignalDFoF']['SignalCalciumImResponseSeries'].data[:]\n",
    "    calc_labels = read_nwb.processing['CalciumActivity']['NeuronIDs'].labels[:]\n",
    "\n",
    "    labels = np.asarray([\"\".join(label) for label in labels])\n",
    "\n",
    "#dfof = fluor / np.mean(fluor, axis=0)\n",
    "\n",
    "print(dfof.shape)\n",
    "print(calc_labels.shape)\n",
    "\n",
    "pca, transform, labels = run_pca(dfof.T, calc_labels, 'AVAR', rate, 3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "Yemini_good_PCA = []\n",
    "\n",
    "for file in os.listdir('/Users/danielysprague/foco_lab/data/Yemini_NWB'):\n",
    "    if not file[-4:] == '.nwb':\n",
    "        continue\n",
    "\n",
    "    print(file)\n",
    "    filepath = '/Users/danielysprague/foco_lab/data/Yemini_NWB/' + file\n",
    "\n",
    "    with NWBHDF5IO(filepath, mode='r', load_namespaces=True) as io:\n",
    "        read_nwb = io.read()\n",
    "        identifier = read_nwb.identifier\n",
    "        seg = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:]\n",
    "        labels = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "        #labels_index = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels_index'][:]\n",
    "        channels = read_nwb.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "        image = read_nwb.acquisition['NeuroPALImageRaw'].data[:]\n",
    "        scale = read_nwb.imaging_planes['NeuroPALImVol'].grid_spacing[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "\n",
    "        rate = read_nwb.acquisition['CalciumImageSeries'].rate\n",
    "        fluor = read_nwb.processing['CalciumActivity']['SignalRawFluor']['SignalCalciumImResponseSeries'].data[:]\n",
    "        calc_labels = read_nwb.processing['CalciumActivity']['NeuronIDs'].labels[:]\n",
    "\n",
    "        labels = np.asarray([\"\".join(label) for label in labels])\n",
    "\n",
    "    dfof = fluor[20:,:]/ np.mean(fluor[20:,:], axis=0)\n",
    "\n",
    "    if not 'AVAL' in calc_labels:\n",
    "        continue\n",
    "\n",
    "    pca, weights, transform, keep_labels = run_pca(dfof.T, calc_labels, 'AVAL', rate, 3)\n",
    "\n",
    "    fig, axs = plt.subplots(3,1, sharex=True)\n",
    "    axs[0].plot(np.linspace(0,transform.shape[0]/rate, transform.shape[0]), transform[:,0])\n",
    "    axs[1].plot(np.linspace(0,transform.shape[0]/rate, transform.shape[0]), transform[:,1])\n",
    "    axs[2].plot(np.linspace(0,transform.shape[0]/rate, transform.shape[0]), transform[:,2])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    x = input('Keep?')\n",
    "\n",
    "    if x == 'y':\n",
    "        Yemini_good_PCA.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "Kimura_good_PCA = []\n",
    "\n",
    "for file in os.listdir('/Users/danielysprague/foco_lab/data/kimura_full'):\n",
    "    if not file[-4:] == '.nwb':\n",
    "        continue\n",
    "\n",
    "    print(file)\n",
    "    filepath = '/Users/danielysprague/foco_lab/data/kimura_full/' + file\n",
    "\n",
    "    with NWBHDF5IO(filepath, mode='r', load_namespaces=True) as io:\n",
    "        read_nwb = io.read()\n",
    "        identifier = read_nwb.identifier\n",
    "        seg = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:]\n",
    "        #labels = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "        #labels_index = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels_index'][:]\n",
    "        channels = read_nwb.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "        image = read_nwb.acquisition['NeuroPALImageRaw'].data[:]\n",
    "        scale = read_nwb.imaging_planes['NeuroPALImVol'].grid_spacing[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "\n",
    "        rate = read_nwb.acquisition['CalciumImageSeries'].rate\n",
    "        fluor = read_nwb.processing['CalciumActivity']['SignalRawFluor']['SignalCalciumImResponseSeries'].data[:]\n",
    "        calc_labels = read_nwb.processing['CalciumActivity']['NeuronIDs'].labels[:]\n",
    "\n",
    "        labels = np.asarray([\"\".join(label) for label in labels])\n",
    "\n",
    "    dfof = fluor[20:,:]/ np.mean(fluor[20:,:], axis=0)\n",
    "\n",
    "    if not 'AVAL' in calc_labels:\n",
    "        continue\n",
    "\n",
    "    pca, weights, transform, keep_labels = run_pca(dfof.T, calc_labels, 'AVAL', rate, 3)\n",
    "\n",
    "    fig, axs = plt.subplots(3,1, sharex=True)\n",
    "    axs[0].plot(np.linspace(0,transform.shape[0]/rate, transform.shape[0]), transform[:,0])\n",
    "    axs[1].plot(np.linspace(0,transform.shape[0]/rate, transform.shape[0]), transform[:,1])\n",
    "    axs[2].plot(np.linspace(0,transform.shape[0]/rate, transform.shape[0]), transform[:,2])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    x = input('Keep?')\n",
    "\n",
    "    if x == 'y':\n",
    "        Kimura_good_PCA.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = []\n",
    "rates = []\n",
    "\n",
    "df = pd.DataFrame(columns=['Neuron', 'PC', 'Weight', 'File'])\n",
    "\n",
    "for file in Yemini_good_PCA:\n",
    "    if not file[-4:] == '.nwb':\n",
    "        continue\n",
    "\n",
    "    print(file)\n",
    "    filepath = '/Users/danielysprague/foco_lab/data/Yemini_nwb/' + file\n",
    "\n",
    "    with NWBHDF5IO(filepath, mode='r', load_namespaces=True) as io:\n",
    "        read_nwb = io.read()\n",
    "        identifier = read_nwb.identifier\n",
    "        seg = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:]\n",
    "        #labels = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "        #labels_index = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels_index'][:]\n",
    "        channels = read_nwb.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "        image = read_nwb.acquisition['NeuroPALImageRaw'].data[:]\n",
    "        scale = read_nwb.imaging_planes['NeuroPALImVol'].grid_spacing[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "\n",
    "        rate = read_nwb.acquisition['CalciumImageSeries'].rate\n",
    "        fluor = read_nwb.processing['CalciumActivity']['SignalRawFluor']['SignalCalciumImResponseSeries'].data[:]\n",
    "        calc_labels = read_nwb.processing['CalciumActivity']['NeuronIDs'].labels[:]\n",
    "\n",
    "        labels = np.asarray([\"\".join(label) for label in labels])\n",
    "\n",
    "    dfof = fluor[20:,:]/ np.mean(fluor[20:,:], axis=0)\n",
    "\n",
    "    if not 'AVAL' in calc_labels:\n",
    "        continue\n",
    "\n",
    "    pca, weights, transform, keep_labels = run_pca(dfof.T, calc_labels, 'AVAL', rate, 3)\n",
    "\n",
    "    #index_AVA = np.argwhere((np.asarray(keep_labels)=='AVAR') | (np.asarray(keep_labels)=='AVAL'))\n",
    "    #index_SMDV = np.argwhere((np.asarray(keep_labels)=='SMDVR') | (np.asarray(keep_labels)=='SMDVL'))\n",
    "    #index_AWC = np.argwhere((np.asarray(keep_labels)=='AWCR') | (np.asarray(keep_labels)=='AWCL'))\n",
    "    #index_ASH = np.argwhere((np.asarray(keep_labels)=='ASHR') | (np.asarray(keep_labels)=='ASHL'))\n",
    "    #index_RID = np.argwhere(np.asarray(keep_labels)=='RID')\n",
    "\n",
    "    for i in range(len(keep_labels)):\n",
    "        label = keep_labels[i]\n",
    "        if label in ['RID', 'RMEV', 'VB2', 'AWCR', 'AWCL']:\n",
    "            df.loc[len(df.index)] = [label, 'PC1', weights[i,0], file[:-4]]\n",
    "            df.loc[len(df.index)] = [label, 'PC2', weights[i,1], file[:-4]]\n",
    "            df.loc[len(df.index)] = [label, 'PC3', weights[i,2], file[:-4]]\n",
    "        elif label in ['SMDVR', 'SMDVL', 'AVAR', 'AVAL']:\n",
    "            df.loc[len(df.index)] = [label[:-1], 'PC1', weights[i,0], file[:-4]]\n",
    "            df.loc[len(df.index)] = [label[:-1], 'PC2', weights[i,1], file[:-4]]\n",
    "            df.loc[len(df.index)] = [label[:-1], 'PC3', weights[i,2], file[:-4]]\n",
    "\n",
    "    pcs.append(transform)\n",
    "    rates.append(rate)\n",
    "\n",
    "for file in Kimura_good_PCA:\n",
    "    if not file[-4:] == '.nwb':\n",
    "        continue\n",
    "\n",
    "    print(file)\n",
    "    filepath = '/Users/danielysprague/foco_lab/data/kimura_full/' + file\n",
    "\n",
    "    with NWBHDF5IO(filepath, mode='r', load_namespaces=True) as io:\n",
    "        read_nwb = io.read()\n",
    "        identifier = read_nwb.identifier\n",
    "        seg = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:]\n",
    "        #labels = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "        #labels_index = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels_index'][:]\n",
    "        channels = read_nwb.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "        image = read_nwb.acquisition['NeuroPALImageRaw'].data[:]\n",
    "        scale = read_nwb.imaging_planes['NeuroPALImVol'].grid_spacing[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "\n",
    "        rate = read_nwb.acquisition['CalciumImageSeries'].rate\n",
    "        fluor = read_nwb.processing['CalciumActivity']['SignalRawFluor']['SignalCalciumImResponseSeries'].data[:]\n",
    "        calc_labels = read_nwb.processing['CalciumActivity']['NeuronIDs'].labels[:]\n",
    "\n",
    "        labels = np.asarray([\"\".join(label) for label in labels])\n",
    "\n",
    "    dfof = fluor[20:,:]/ np.mean(fluor[20:,:], axis=0)\n",
    "\n",
    "    if not 'AVAL' in calc_labels:\n",
    "        continue\n",
    "\n",
    "    pca, weights, transform, keep_labels = run_pca(dfof.T, calc_labels, 'AVAL', rate, 3)\n",
    "\n",
    "    #index_AVA = np.argwhere((np.asarray(keep_labels)=='AVAR') | (np.asarray(keep_labels)=='AVAL'))\n",
    "    #index_SMDV = np.argwhere((np.asarray(keep_labels)=='SMDVR') | (np.asarray(keep_labels)=='SMDVL'))\n",
    "    #index_AWC = np.argwhere((np.asarray(keep_labels)=='AWCR') | (np.asarray(keep_labels)=='AWCL'))\n",
    "    #index_ASH = np.argwhere((np.asarray(keep_labels)=='ASHR') | (np.asarray(keep_labels)=='ASHL'))\n",
    "    #index_RID = np.argwhere(np.asarray(keep_labels)=='RID')\n",
    "\n",
    "    for i in range(len(keep_labels)):\n",
    "        label = keep_labels[i]\n",
    "        if label in ['RID', 'RMEV', 'VB2', 'AWCR', 'AWCL']:\n",
    "            df.loc[len(df.index)] = [label, 'PC1', weights[i,0], file[:-4]]\n",
    "            df.loc[len(df.index)] = [label, 'PC2', weights[i,1], file[:-4]]\n",
    "            df.loc[len(df.index)] = [label, 'PC3', weights[i,2], file[:-4]]\n",
    "        elif label in ['SMDVR', 'SMDVL', 'AVAR', 'AVAL']:\n",
    "            df.loc[len(df.index)] = [label[:-1], 'PC1', weights[i,0], file[:-4]]\n",
    "            df.loc[len(df.index)] = [label[:-1], 'PC2', weights[i,1], file[:-4]]\n",
    "            df.loc[len(df.index)] = [label[:-1], 'PC3', weights[i,2], file[:-4]]\n",
    "\n",
    "    pcs.append(transform)\n",
    "    rates.append(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yem_files = [file[:-4] for file in Yemini_good_PCA]\n",
    "kim_files = [file[:-4] for file in Kimura_good_PCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plt.figure()\n",
    "sns.boxplot(data=df[df['File'].isin(kim_files)], x='Neuron', y='Weight', hue='PC', order= ['RID', 'AVA', 'SMDV', 'RMEV', 'VB2', 'AWCR', 'AWCL'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = []\n",
    "rates = []\n",
    "\n",
    "df = pd.DataFrame(columns=['Neuron', 'PC', 'Weight', 'File'])\n",
    "\n",
    "for folder in ['Yemini_NWB', 'Kimura_full']:\n",
    "    for file in os.listdir('/Users/danielysprague/foco_lab/data/'+folder):\n",
    "        if not file[-4:] == '.nwb':\n",
    "            continue\n",
    "\n",
    "        print(file)\n",
    "        filepath = '/Users/danielysprague/foco_lab/data/' + folder+ '/' + file\n",
    "\n",
    "        with NWBHDF5IO(filepath, mode='r', load_namespaces=True) as io:\n",
    "            read_nwb = io.read()\n",
    "            identifier = read_nwb.identifier\n",
    "            seg = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:]\n",
    "            #labels = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "            #labels_index = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels_index'][:]\n",
    "            channels = read_nwb.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "            image = read_nwb.acquisition['NeuroPALImageRaw'].data[:]\n",
    "            scale = read_nwb.imaging_planes['NeuroPALImVol'].grid_spacing[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "\n",
    "            rate = read_nwb.acquisition['CalciumImageSeries'].rate\n",
    "            fluor = read_nwb.processing['CalciumActivity']['SignalRawFluor']['SignalCalciumImResponseSeries'].data[:]\n",
    "            calc_labels = read_nwb.processing['CalciumActivity']['NeuronIDs'].labels[:]\n",
    "\n",
    "            labels = np.asarray([\"\".join(label) for label in labels])\n",
    "\n",
    "        dfof = (fluor[20:,:] -np.mean(fluor[20:,:]))/ np.mean(fluor[20:,:], axis=0)\n",
    "\n",
    "        if not 'AVAL' in calc_labels:\n",
    "            continue\n",
    "\n",
    "        pca, weights, transform, keep_labels = run_pca(dfof.T, calc_labels, 'AVAL', rate, 3)\n",
    "\n",
    "        #index_AVA = np.argwhere((np.asarray(keep_labels)=='AVAR') | (np.asarray(keep_labels)=='AVAL'))\n",
    "        #index_SMDV = np.argwhere((np.asarray(keep_labels)=='SMDVR') | (np.asarray(keep_labels)=='SMDVL'))\n",
    "        #index_AWC = np.argwhere((np.asarray(keep_labels)=='AWCR') | (np.asarray(keep_labels)=='AWCL'))\n",
    "        #index_ASH = np.argwhere((np.asarray(keep_labels)=='ASHR') | (np.asarray(keep_labels)=='ASHL'))\n",
    "        #index_RID = np.argwhere(np.asarray(keep_labels)=='RID')\n",
    "\n",
    "        for i in range(len(keep_labels)):\n",
    "            label = keep_labels[i]\n",
    "            if label in ['RID', 'RMEV', 'VB2', 'AWCR', 'AWCL']:\n",
    "                df.loc[len(df.index)] = [label, 'PC1', weights[i,0], file[:-4]]\n",
    "                df.loc[len(df.index)] = [label, 'PC2', weights[i,1], file[:-4]]\n",
    "                df.loc[len(df.index)] = [label, 'PC3', weights[i,2], file[:-4]]\n",
    "            elif label in ['SMDVR', 'SMDVL', 'AVAR', 'AVAL']:\n",
    "                df.loc[len(df.index)] = [label[:-1], 'PC1', weights[i,0], file[:-4]]\n",
    "                df.loc[len(df.index)] = [label[:-1], 'PC2', weights[i,1], file[:-4]]\n",
    "                df.loc[len(df.index)] = [label[:-1], 'PC3', weights[i,2], file[:-4]]\n",
    "\n",
    "        pcs.append(transform)\n",
    "        rates.append(rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.boxplot(data=df, x='Neuron', y='Weight', hue='PC', order= ['RID', 'AVA', 'SMDV', 'RMEV', 'VB2', 'AWCR', 'AWCL'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs  = plt.subplots(3,5)\n",
    "for i in range(5):\n",
    "    axs[0,i].plot(np.linspace(0,pcs[i].shape[0]/rates[i], pcs[i].shape[0]), pcs[i][:,0])\n",
    "    axs[1,i].plot(np.linspace(0,pcs[i].shape[0]/rates[i], pcs[i].shape[0]), pcs[i][:,1])\n",
    "    axs[2,i].plot(np.linspace(0,pcs[i].shape[0]/rates[i], pcs[i].shape[0]), pcs[i][:,2])\n",
    "    axs[0,0].set_ylabel('PC1')\n",
    "    axs[1,0].set_ylabel('PC2')\n",
    "    axs[2,0].set_ylabel('PC3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs  = plt.subplots(3,5)\n",
    "for i in range(5):\n",
    "    axs[0,i].plot(np.linspace(0,pcs[i+21].shape[0]/rates[i+21], pcs[i+21].shape[0]), pcs[i+21][:,0])\n",
    "    axs[1,i].plot(np.linspace(0,pcs[i+21].shape[0]/rates[i+21], pcs[i+21].shape[0]), pcs[i+21][:,1])\n",
    "    axs[2,i].plot(np.linspace(0,pcs[i+21].shape[0]/rates[i+21], pcs[i+21].shape[0]), pcs[i+21][:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "traces = np.transpose(fluor)\n",
    "\n",
    "labels = [\"\".join(label) for label in labels]\n",
    "\n",
    "def plot_traces(traces, rate, labels, selected):\n",
    "\n",
    "    seconds = traces.shape[1]//rate\n",
    "\n",
    "    fig, axs = plt.subplots(len(selected),1, figsize=(5,6))\n",
    "\n",
    "    for i, neuron in enumerate(selected):\n",
    "        index = np.argwhere(np.asarray(labels)==neuron)\n",
    "        trace = traces[np.squeeze(index),:]\n",
    "\n",
    "        axs[i].plot(np.linspace(0,seconds,traces.shape[1]), trace)\n",
    "        axs[i].set_xlabel('seconds')\n",
    "        axs[i].set_ylabel('DFoF')\n",
    "        axs[i].set_xlim(0,seconds)\n",
    "        axs[i].set_title(neuron)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_traces(traces, rate, labels, ['AVAR', 'SMDVR', 'AWCR','RID', 'ASHR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wavelet_decomp(signal, waveletname, level):\n",
    "    fig, ax = plt.subplots(figsize=(6,1))\n",
    "    ax.set_title(\"Original Signal: \")\n",
    "    ax.plot(signal)\n",
    "    plt.show()\n",
    "        \n",
    "    data = signal\n",
    "\n",
    "    detail_coefs = []\n",
    "    \n",
    "    fig, axarr = plt.subplots(nrows=5, ncols=2, figsize=(6,6))\n",
    "    for ii in range(level):\n",
    "        (data, coeff_d) = pywt.dwt(data, waveletname)\n",
    "        detail_coefs.append(coeff_d)\n",
    "        axarr[ii, 0].plot(data, 'r')\n",
    "        axarr[ii, 1].plot(coeff_d, 'g')\n",
    "        axarr[ii, 0].set_ylabel(\"Level {}\".format(ii + 1), fontsize=14, rotation=90)\n",
    "        axarr[ii, 0].set_yticklabels([])\n",
    "        if ii == 0:\n",
    "            axarr[ii, 0].set_title(\"Approximation coefficients\", fontsize=14)\n",
    "            axarr[ii, 1].set_title(\"Detail coefficients\", fontsize=14)\n",
    "        axarr[ii, 1].set_yticklabels([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return detail_coefs\n",
    "\n",
    "coefs_db5 = plot_wavelet_decomp(trace, 'db5', 5)\n",
    "coefs_sym5 = plot_wavelet_decomp(trace, 'sym5', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db5_power = []\n",
    "sym5_power = []\n",
    "\n",
    "for i in range(len(coefs_db5)):\n",
    "    db5_power.append(np.sqrt(np.mean(coefs_db5[i]**2)))\n",
    "    sym5_power.append(np.sqrt(np.mean(coefs_sym5[i]**2)))\n",
    "\n",
    "plt.plot(2**np.linspace(0,4,5),db5_power)\n",
    "plt.plot(2**np.linspace(0,4,5),sym5_power)\n",
    "plt.xlabel('Scale of wavelet (s)')\n",
    "plt.ylabel('RMS power of detail coefficients')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/danielysprague/foco_lab/data/kimura_full/sub-230928-02_ses-20230928T111400_ophys.nwb'\n",
    "\n",
    "with NWBHDF5IO(filepath, mode='r', load_namespaces=True) as io:\n",
    "    read_nwb = io.read()\n",
    "    seg = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:]\n",
    "    labels = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "    channels = read_nwb.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "    image = read_nwb.acquisition['NeuroPALImageRaw'].data[:]\n",
    "    scale = read_nwb.imaging_planes['NeuroPALImVol'].grid_spacing[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "\n",
    "    fluor = read_nwb.processing['CalciumActivity']['SignalRawFluor']['SignalCalciumImResponseSeries'].data[:]\n",
    "    calc_labels = read_nwb.processing['CalciumActivity']['NeuronIDs'].labels[:]\n",
    "\n",
    "print(labels)\n",
    "print(calc_labels)\n",
    "\n",
    "read_nwb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sio.loadmat('/Users/danielysprague/foco_lab/data/Yemini_21/OH16230/Heads/20190924_01/')['gclabels']\n",
    "gclabels = np.asarray([label.replace(\" \",\"\") for label in labels])\n",
    "print(gclabels.shape)\n",
    "coefs_sym5 = plot_wavelet_decomp(fluor[:,0], 'sym5', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "AVA_traces = []\n",
    "SMDV_traces = []\n",
    "AWC_traces = []\n",
    "ASH_traces = []\n",
    "RID_traces = []\n",
    "\n",
    "for file in os.listdir('/Users/danielysprague/foco_lab/data/kimura_full'):\n",
    "    if not file[-4:] == '.nwb':\n",
    "        continue\n",
    "\n",
    "    filepath = '/Users/danielysprague/foco_lab/data/kimura_full/' + file\n",
    "\n",
    "    with NWBHDF5IO(filepath, mode='r', load_namespaces=True) as io:\n",
    "        read_nwb = io.read()\n",
    "        seg = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:]\n",
    "        labels = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "        channels = read_nwb.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "        image = read_nwb.acquisition['NeuroPALImageRaw'].data[:]\n",
    "        scale = read_nwb.imaging_planes['NeuroPALImVol'].grid_spacing[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "\n",
    "        fluor = read_nwb.processing['CalciumActivity']['SignalRawFluor']['SignalCalciumImResponseSeries'].data[:]\n",
    "        calc_labels = read_nwb.processing['CalciumActivity']['NeuronIDs'].labels[:]\n",
    "        rate = read_nwb.acquisition['CalciumImageSeries'].rate\n",
    "\n",
    "    index_AVA = np.argwhere((np.asarray(calc_labels)=='AVAR') | (np.asarray(calc_labels)=='AVAL'))\n",
    "    index_SMDV = np.argwhere((np.asarray(calc_labels)=='SMDVR') | (np.asarray(calc_labels)=='SMDVL'))\n",
    "    index_AWC = np.argwhere((np.asarray(calc_labels)=='AWCR') | (np.asarray(calc_labels)=='AWCL'))\n",
    "    index_ASH = np.argwhere((np.asarray(calc_labels)=='ASHR') | (np.asarray(calc_labels)=='ASHL'))\n",
    "    index_RID = np.argwhere(np.asarray(calc_labels)=='RID')\n",
    "    AVA_traces = AVA_traces + [np.squeeze(fluor[:,index]/np.mean(fluor[:,index])) for index in index_AVA if index is not None]\n",
    "    SMDV_traces = SMDV_traces + [np.squeeze(fluor[:,index]/np.mean(fluor[:,index])) for index in index_SMDV if index is not None]\n",
    "    AWC_traces = AWC_traces + [np.squeeze(fluor[:, index]/np.mean(fluor[:,index])) for index in index_AWC if index is not None]\n",
    "    ASH_traces = ASH_traces + [np.squeeze(fluor[:,index]/np.mean(fluor[:,index])) for index in index_ASH if index is not None]\n",
    "    RID_traces = RID_traces + [np.squeeze(fluor[:,index]/np.mean(fluor[:,index])) for index in index_RID if index is not None]\n",
    "\n",
    "#for i, trace in enumerate(AVA_traces):\n",
    "#    plt.plot(np.linspace(0,985,1645), trace[:1645], alpha=0.5)\n",
    "#plt.show()\n",
    "\n",
    "#for i, trace in enumerate(AVA_traces):\n",
    "#    plt.plot(np.fft.fft(trace[:1645], axis=0), alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "AVA_traces = []\n",
    "SMDV_traces = []\n",
    "AWC_traces = []\n",
    "ASH_traces = []\n",
    "RID_traces = []\n",
    "\n",
    "for file in os.listdir('/Users/danielysprague/foco_lab/data/Yemini_NWB'):\n",
    "    if not file[-4:] == '.nwb':\n",
    "        continue\n",
    "\n",
    "    filepath = '/Users/danielysprague/foco_lab/data/Yemini_NWB/' + file\n",
    "\n",
    "    with NWBHDF5IO(filepath, mode='r', load_namespaces=True) as io:\n",
    "        read_nwb = io.read()\n",
    "        seg = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:]\n",
    "        labels = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "        channels = read_nwb.acquisition['NeuroPALImageRaw'].RGBW_channels[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "        image = read_nwb.acquisition['NeuroPALImageRaw'].data[:]\n",
    "        scale = read_nwb.imaging_planes['NeuroPALImVol'].grid_spacing[:] #get which channels of the image correspond to which RGBW pseudocolors\n",
    "\n",
    "        fluor = read_nwb.processing['CalciumActivity']['SignalRawFluor']['SignalCalciumImResponseSeries'].data[:]\n",
    "        calc_labels = read_nwb.processing['CalciumActivity']['NeuronIDs'].labels[:]\n",
    "        rate = read_nwb.acquisition['CalciumImageSeries'].rate\n",
    "\n",
    "    index_AVA = np.argwhere((np.asarray(calc_labels)=='AVAR') | (np.asarray(calc_labels)=='AVAL'))\n",
    "    index_SMDV = np.argwhere((np.asarray(calc_labels)=='SMDVR') | (np.asarray(calc_labels)=='SMDVL'))\n",
    "    index_AWC = np.argwhere((np.asarray(calc_labels)=='AWCR') | (np.asarray(calc_labels)=='AWCL'))\n",
    "    index_ASH = np.argwhere((np.asarray(calc_labels)=='ASHR') | (np.asarray(calc_labels)=='ASHL'))\n",
    "    index_RID = np.argwhere(np.asarray(calc_labels)=='RID')\n",
    "    AVA_traces = AVA_traces + [np.squeeze(fluor[:,index]/np.mean(fluor[:,index])) for index in index_AVA if index is not None]\n",
    "    SMDV_traces = SMDV_traces + [np.squeeze(fluor[:,index]/np.mean(fluor[:,index])) for index in index_SMDV if index is not None]\n",
    "    AWC_traces = AWC_traces + [np.squeeze(fluor[:, index]/np.mean(fluor[:,index])) for index in index_AWC if index is not None]\n",
    "    ASH_traces = ASH_traces + [np.squeeze(fluor[:,index]/np.mean(fluor[:,index])) for index in index_ASH if index is not None]\n",
    "    RID_traces = RID_traces + [np.squeeze(fluor[:,index]/np.mean(fluor[:,index])) for index in index_RID if index is not None]\n",
    "\n",
    "#for i, trace in enumerate(AVA_traces):\n",
    "#    plt.plot(np.linspace(0,985,1645), trace[:1645], alpha=0.5)\n",
    "#plt.show()\n",
    "\n",
    "#for i, trace in enumerate(AVA_traces):\n",
    "#    plt.plot(np.fft.fft(trace[:1645], axis=0), alpha=0.5)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wavelet_decomp(signal, waveletname, level):\n",
    "    fig, ax = plt.subplots(figsize=(6,1))\n",
    "    ax.set_title(\"Original Signal: \")\n",
    "    ax.plot(signal)\n",
    "    plt.show()\n",
    "        \n",
    "    data = signal\n",
    "\n",
    "    detail_coefs = []\n",
    "    \n",
    "    fig, axarr = plt.subplots(nrows=level, ncols=2, figsize=(6,6))\n",
    "    for ii in range(level):\n",
    "        (data, coeff_d) = pywt.dwt(data, waveletname)\n",
    "        detail_coefs.append(coeff_d)\n",
    "        axarr[ii, 0].plot(data, 'r')\n",
    "        axarr[ii, 1].plot(coeff_d, 'g')\n",
    "        axarr[ii, 0].set_ylabel(\"Level {}\".format(ii + 1), fontsize=14, rotation=90)\n",
    "        axarr[ii, 0].set_yticklabels([])\n",
    "        if ii == 0:\n",
    "            axarr[ii, 0].set_title(\"Approximation coefficients\", fontsize=14)\n",
    "            axarr[ii, 1].set_title(\"Detail coefficients\", fontsize=14)\n",
    "        axarr[ii, 1].set_yticklabels([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return detail_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wavelet_decomp(signal, waveletname, level):\n",
    "\n",
    "    detail_coefs = []\n",
    "    approx_coefs = []\n",
    "    detail_power = []\n",
    "    \n",
    "    data=signal\n",
    "\n",
    "    for ii in range(level):\n",
    "        (data, coeffs_d) = pywt.dwt(data, waveletname)\n",
    "        approx_coefs.append(data)\n",
    "        detail_coefs.append(coeffs_d)\n",
    "        detail_power.append(np.sqrt(np.mean(coeffs_d**2)))\n",
    "\n",
    "    return approx_coefs, detail_coefs, detail_power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wavelet_decomp(fluor[:,5], 'sym5', 5)\n",
    "plot_wavelet_decomp(AVA_traces[10], 'haar', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "wavelet = pywt.Wavelet('haar')\n",
    "[dec_lo, dec_hi, rec_lo, rec_hi] = wavelet.filter_bank\n",
    "print(wavelet.dec_len)\n",
    "\n",
    "print(dec_lo)\n",
    "print(dec_hi)\n",
    "\n",
    "plt.plot(dec_lo)\n",
    "plt.plot(dec_hi)\n",
    "plt.legend(labels=['decomp low', 'decomp high'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "waveletname = 'haar'\n",
    "wavelet = pywt.Wavelet(waveletname)\n",
    "wavelen = wavelet.dec_len\n",
    "level = 7\n",
    "\n",
    "rate= rate\n",
    "length = 936\n",
    "\n",
    "neurons = ['AVA', 'SMDV', 'AWC', 'ASH', 'RID']\n",
    "\n",
    "all_traces = [AVA_traces, SMDV_traces, AWC_traces, ASH_traces, RID_traces]\n",
    "\n",
    "df = pd.DataFrame(columns=['Neuron_name', 'DecompLevel', 'Power'])\n",
    "\n",
    "for i, neuron in enumerate(neurons):\n",
    "    for j in range(len(all_traces[i])):\n",
    "        approx, detail, power = get_wavelet_decomp(np.squeeze(all_traces[i][j][50:]), waveletname, level)\n",
    "        df = pd.concat([df, pd.DataFrame([{'neuron_name': neuron, 'DecompScale': str((wavelen/rate)* 2**k)[:4], 'Power':power[k]} for k in range(len(power))])])\n",
    "\n",
    "    '''\n",
    "    AVA_approx, AVA_detail, AVA_power = get_wavelet_decomp(AVA_traces[i], waveletname, level)\n",
    "    SMDV_approx, SMDV_detail, SMDV_power = get_wavelet_decomp(SMDV_traces[i], waveletname, level)\n",
    "    AWC_approx, AWC_detail, AWC_power = get_wavelet_decomp(AWC_traces[i], waveletname, level)\n",
    "    ASH_approx, ASH_detail, ASH_power = get_wavelet_decomp(ASH_traces[i], waveletname, level)\n",
    "    RID_approx, RID_detail, RID_power = get_wavelet_decomp(RID_traces[i], waveletname, level)\n",
    "    \n",
    "    df = pd.concat([df, pd.DataFrame([{'neuron_name':'AVA', 'DecompLevel':2**j, 'Power':AVA_power[j]} for j in range(len(AVA_power))])])\n",
    "    df = pd.concat([df, pd.DataFrame([{'neuron_name':'SMDV', 'DecompLevel':2**j, 'Power':SMDV_power[j]} for j in range(len(SMDV_power))])])\n",
    "    df = pd.concat([df, pd.DataFrame([{'neuron_name':'AWC', 'DecompLevel':2**j, 'Power':AWC_power[j]} for j in range(len(AWC_power))])])\n",
    "    df = pd.concat([df, pd.DataFrame([{'neuron_name':'ASH', 'DecompLevel':2**j, 'Power':ASH_power[j]} for j in range(len(ASH_power))])])\n",
    "    df = pd.concat([df, pd.DataFrame([{'neuron_name':'RID', 'DecompLevel':2**j, 'Power':RID_power[j]} for j in range(len(RID_power))])])\n",
    "    '''\n",
    "\n",
    "fig, axs = plt.subplots(5,2)\n",
    "\n",
    "for i, neuron in enumerate(neurons):\n",
    "    for j in range(len(all_traces[i])):\n",
    "        axs[i,0].plot(np.linspace(50,length/rate,length-50), all_traces[i][j][50:length], alpha=0.5)\n",
    "    axs[i,0].set_xlabel('Time in seconds')\n",
    "    axs[i,0].set_ylabel('df/f')\n",
    "    axs[i,0].title.set_text(neuron)\n",
    "\n",
    "    sns.boxplot(ax=axs[i,1], data= df[df['neuron_name']==neuron], x='DecompScale', y='Power')\n",
    "    axs[i,1].set_xlabel('Wavelet scale in seconds')\n",
    "    axs[i,1].set_ylabel('Wavelet power')\n",
    "    axs[i,1].title.set_text(neuron)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "def plot_accuracies(datasets, accs_NP, accs_full, labels):\n",
    "\n",
    "    df_dataset = pd.DataFrame(columns=['Atlas', 'Dataset', 'Accuracy'])\n",
    "    df_ranks = pd.DataFrame(columns= ['Atlas', 'Rank', 'Accuracy'])\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for key in dataset.keys():\n",
    "            acc_NP = accs_NP.loc[accs_NP['Filename']==key]\n",
    "            acc_full = accs_full.loc[accs_full['Filename']==key]\n",
    "\n",
    "            df_dataset.loc[len(df_dataset.index)] = ['NP', labels[i], acc_NP.iloc[0]['Percent_top1']]\n",
    "            df_dataset.loc[len(df_dataset.index)] = ['Consolidated', labels[i], acc_full.iloc[0]['Percent_top1']]\n",
    "\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['NP', 'top', acc_NP.iloc[0]['Percent_top1']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Consolidated', 'top', acc_full.iloc[0]['Percent_top1']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['NP', 'top2', acc_NP.iloc[0]['Percent_top2']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Consolidated', 'top2', acc_full.iloc[0]['Percent_top2']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['NP', 'top3', acc_NP.iloc[0]['Percent_top3']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Consolidated', 'top3', acc_full.iloc[0]['Percent_top3']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['NP', 'top4', acc_NP.iloc[0]['Percent_top4']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Consolidated', 'top4', acc_full.iloc[0]['Percent_top4']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['NP', 'top5', acc_NP.iloc[0]['Percent_top5']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Consolidated', 'top5', acc_full.iloc[0]['Percent_top5']]\n",
    "\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "\n",
    "    sns.set(style='white', font_scale=1.5)\n",
    "\n",
    "    sns.violinplot(ax= axs[0], data = df_dataset, x = 'Dataset', y='Accuracy', hue='Atlas', gap=0.5, palette=['purple','pink'], orient='v', split=True, cut=0, inner='quart', density_norm='width') \n",
    "    \n",
    "    for i, dataset in enumerate(labels):\n",
    "        NP_vals = df_dataset[(df_dataset['Dataset']==dataset)&(df_dataset['Atlas']=='NP')]['Accuracy']\n",
    "        consol_vals = df_dataset[(df_dataset['Dataset']==dataset)&(df_dataset['Atlas']=='Consolidated')]['Accuracy']\n",
    "\n",
    "        for a, b in zip(NP_vals, consol_vals):\n",
    "            axs[0].plot([i-0.1,i+0.1], [a,b], color='black', linewidth=0.5)\n",
    "\n",
    "    sns.violinplot(ax= axs[1], data = df_ranks, x = 'Rank', y='Accuracy', hue='Atlas', gap=0.5, palette=['purple', 'pink'], orient='v', split=True, cut=0, inner='quart', density_norm='width') \n",
    "\n",
    "    ranks = ['top', 'top2', 'top3', 'top4', 'top5']\n",
    "\n",
    "    for j, rank in enumerate(ranks):\n",
    "        NP_vals_rank = df_ranks[(df_ranks['Rank']==rank)&(df_ranks['Atlas']=='NP')]['Accuracy']\n",
    "        consol_vals_rank = df_ranks[(df_ranks['Rank']==rank)&(df_ranks['Atlas']=='Consolidated')]['Accuracy']\n",
    "\n",
    "        for a, b in zip(NP_vals_rank, consol_vals_rank):\n",
    "            axs[1].plot([j-0.1,j+0.1], [a,b], color='black', linewidth=0.5)\n",
    "\n",
    "    axs[0].set_ylim((0,1))\n",
    "    axs[1].set_ylim((0,1))\n",
    "    \n",
    "    axs[0].set_title('Accuracy by dataset')\n",
    "    axs[1].set_title('Cumulative accuracy of top n assignments')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "#plot_accuracies([chaud_dataset, Yem_dataset, old_FOCO_dataset, FOCO_dataset, kimura_dataset, flavell_dataset], accs_NP, accs_full, ['1', '2', '3', '4', '5', '6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def gen_plots_acc(datasets, labels,accs_NP_unmatch, accs_NP, accs_full_unmatch, accs_full):\n",
    "\n",
    "    plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "    df_dataset = pd.DataFrame(columns=['Atlas', 'Dataset', 'Accuracy'])\n",
    "    df_ranks = pd.DataFrame(columns= ['Atlas', 'Rank', 'Accuracy'])\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for key in dataset.keys():\n",
    "            acc_NP_unmatch = accs_NP_unmatch.loc[accs_NP_unmatch['Filename']==key]\n",
    "            acc_NP = accs_NP.loc[accs_NP['Filename']==key]\n",
    "            acc_full_unmatch = accs_full_unmatch.loc[accs_full_unmatch['Filename']==key]\n",
    "            acc_full = accs_full.loc[accs_full['Filename']==key]\n",
    "\n",
    "            df_dataset.loc[len(df_dataset.index)] = ['Base', labels[i], acc_NP_unmatch.iloc[0]['Percent_top1']]\n",
    "            df_dataset.loc[len(df_dataset.index)] = ['Matched', labels[i], acc_NP.iloc[0]['Percent_top1']]\n",
    "            df_dataset.loc[len(df_dataset.index)] = ['Full', labels[i], acc_full_unmatch.iloc[0]['Percent_top1']]\n",
    "            df_dataset.loc[len(df_dataset.index)] = ['Full matched', labels[i], acc_full.iloc[0]['Percent_top1']]\n",
    "\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['NP', 'top', acc_NP_unmatch.iloc[0]['Percent_top1']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Consolidated', 'top', acc_full.iloc[0]['Percent_top1']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['NP', 'top2', acc_NP_unmatch.iloc[0]['Percent_top2']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Consolidated', 'top2', acc_full.iloc[0]['Percent_top2']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['NP', 'top3', acc_NP_unmatch.iloc[0]['Percent_top3']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Consolidated', 'top3', acc_full.iloc[0]['Percent_top3']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['NP', 'top4', acc_NP_unmatch.iloc[0]['Percent_top4']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Consolidated', 'top4', acc_full.iloc[0]['Percent_top4']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['NP', 'top5', acc_NP_unmatch.iloc[0]['Percent_top5']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Consolidated', 'top5', acc_full.iloc[0]['Percent_top5']]\n",
    "\n",
    "    palette = sns.color_palette('colorblind')\n",
    "    color1 = palette[3]\n",
    "    color2 = palette[2]\n",
    "    color3 = palette[0]\n",
    "    color4 = palette[8]\n",
    "    color5 = palette[4]\n",
    "    color6 = palette[6]\n",
    "\n",
    "    fig, axs = plt.subplots(3,2)\n",
    "\n",
    "    sns.set(style='white', font_scale=1.5)\n",
    "\n",
    "    #sns.violinplot(ax=axs[0][0], data = df_dataset[df_dataset['Atlas']=='NP_unmatch'], x='Dataset', y='Accuracy', palette=['purple'], inner='quart', density_norm='width')\n",
    "\n",
    "    sns.violinplot(ax = axs[0][0], data=df_dataset, x='Atlas', y='Accuracy', hue='Atlas', palette=[color1, color2, color3, color4], cut=0, inner='quart', density_norm='width')\n",
    "\n",
    "    sns.violinplot(ax= axs[0][1], data = df_ranks, x = 'Rank', y='Accuracy', hue='Atlas', gap=0.5, palette=[color1, color4], orient='v', split=True, cut=0, inner='quart', density_norm='width') \n",
    "\n",
    "    ranks = ['top', 'top2', 'top3', 'top4', 'top5']\n",
    "\n",
    "    for j, rank in enumerate(ranks):\n",
    "        NP_vals_rank = df_ranks[(df_ranks['Rank']==rank)&(df_ranks['Atlas']=='NP')]['Accuracy']\n",
    "        consol_vals_rank = df_ranks[(df_ranks['Rank']==rank)&(df_ranks['Atlas']=='Consolidated')]['Accuracy']\n",
    "\n",
    "        for a, b in zip(NP_vals_rank, consol_vals_rank):\n",
    "            axs[0][1].plot([j-0.1,j+0.1], [a,b], color='black', linewidth=0.5)\n",
    "\n",
    "    sns.violinplot(ax= axs[1][0], data = df_dataset[(df_dataset['Atlas']=='Base')|(df_dataset['Atlas']=='Matched')], x = 'Dataset', y='Accuracy', hue='Atlas', gap=0.5, palette=[color1, color2], orient='v', split=True, cut=0, inner='quart', density_norm='width') \n",
    "    sns.violinplot(ax= axs[1][1], data = df_dataset[(df_dataset['Atlas']=='Base')|(df_dataset['Atlas']=='Full')], x = 'Dataset', y='Accuracy', hue='Atlas', gap=0.5,  palette=[color1, color3], orient='v', split=True, cut=0, inner='quart', density_norm='width') \n",
    "    sns.violinplot(ax= axs[2][0], data = df_dataset[(df_dataset['Atlas']=='Full')|(df_dataset['Atlas']=='Full matched')], x = 'Dataset', y='Accuracy', hue='Atlas', gap=0.5, palette=[color3, color4], orient='v', split=True, cut=0, inner='quart', density_norm='width') \n",
    "    sns.violinplot(ax= axs[2][1], data = df_dataset[(df_dataset['Atlas']=='Base')|(df_dataset['Atlas']=='Full matched')], x = 'Dataset', y='Accuracy', hue='Atlas', gap=0.5, palette=[color1, color4], orient='v', split=True, cut=0, inner='quart', density_norm='width') \n",
    "    \n",
    "    for i, dataset in enumerate(labels):\n",
    "        NP_unmatch_vals = df_dataset[(df_dataset['Dataset']==dataset)&(df_dataset['Atlas']=='Base')]['Accuracy']\n",
    "        NP_vals = df_dataset[(df_dataset['Dataset']==dataset)&(df_dataset['Atlas']=='Matched')]['Accuracy']\n",
    "        consol_unmatch_vals = df_dataset[(df_dataset['Dataset']==dataset)&(df_dataset['Atlas']=='Full')]['Accuracy']\n",
    "        consol_vals = df_dataset[(df_dataset['Dataset']==dataset)&(df_dataset['Atlas']=='Full matched')]['Accuracy']\n",
    "\n",
    "        for a, b, c, d in zip(NP_unmatch_vals, NP_vals, consol_unmatch_vals, consol_vals):\n",
    "            axs[1][0].plot([i-0.1,i+0.1], [a,b], color='black', linewidth=0.5)\n",
    "            axs[1][1].plot([i-0.1,i+0.1], [a,c], color='black', linewidth=0.5)\n",
    "            axs[2][0].plot([i-0.1,i+0.1], [c,d], color='black', linewidth=0.5)\n",
    "            axs[2][1].plot([i-0.1,i+0.1], [a,d], color='black', linewidth=0.5)\n",
    "\n",
    "    axs[0][0].set_ylim((0,1))\n",
    "    axs[0][0].set(xlabel=None)\n",
    "    #axs[0][1].legend([],[], frameon=False)\n",
    "    axs[0][1].set_ylim((0,1))\n",
    "    axs[0][1].set_ylabel(None)\n",
    "    axs[0][1].set(xlabel=None)\n",
    "    axs[0][1].legend([],[], frameon=False)\n",
    "    axs[1][0].set_ylim((0,1))\n",
    "    axs[1][0].legend([],[], frameon=False)\n",
    "    axs[1][1].set_ylim((0,1))\n",
    "    axs[1][1].legend([],[], frameon=False)\n",
    "    axs[1][1].set_ylabel(None)\n",
    "    axs[2][0].set_ylim((0,1))\n",
    "    axs[2][0].legend([],[], frameon=False)\n",
    "    axs[2][0].set_xlabel(None)\n",
    "    axs[2][0].set_xticklabels(['Original', 'Color matched', 'Consolidated', 'Consolidated & color matched'])\n",
    "    axs[2][1].set_ylim((0,1))\n",
    "    #axs[2][1].legend([],[], frameon=False)\n",
    "    axs[2][1].set_ylabel(None)\n",
    "    axs[2][1].set_xlabel(None)\n",
    "\n",
    "    base_accs = np.asarray(df_dataset[df_dataset['Atlas']=='Base']['Accuracy'])\n",
    "    match_accs = np.asarray(df_dataset[df_dataset['Atlas']=='Matched']['Accuracy'])\n",
    "    consol_accs = np.asarray(df_dataset[df_dataset['Atlas']=='Full']['Accuracy'])\n",
    "    consol_match_accs = np.asarray(df_dataset[df_dataset['Atlas']=='Full matched']['Accuracy'])\n",
    "\n",
    "    base_match = scipy.stats.ttest_rel(base_accs, match_accs)\n",
    "    base_full = scipy.stats.ttest_rel(base_accs, consol_accs)\n",
    "    full_fullmatch = scipy.stats.ttest_rel(consol_accs, consol_match_accs)\n",
    "    base_fullmatch = scipy.stats.ttest_rel(base_accs, consol_match_accs)\n",
    "\n",
    "    print('t-value: ' +str(base_match.statistic)+' pvalue: '+str(base_match.pvalue))\n",
    "    print('t-value: ' +str(base_full.statistic)+' pvalue: '+str(base_full.pvalue))\n",
    "    print('t-value: ' +str(full_fullmatch.statistic)+' pvalue: '+str(full_fullmatch.pvalue))\n",
    "    print('t-value: ' +str(base_fullmatch.statistic)+' pvalue: '+str(base_fullmatch.pvalue))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "gen_plots_acc([chaud_dataset, Yem_dataset, old_FOCO_dataset, FOCO_dataset, kimura_dataset, flavell_dataset], ['1', '2', '3', '4', '5', '6'], accs_NP_unmatch, accs_NP, accs_full_unmatch, accs_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "\n",
    "def plot_accuracies_atlas_compare(datasets, accs_NP, accs_full):\n",
    "\n",
    "    df = pd.DataFrame(columns=['NeuroPAL','Consolidated', 'dataset'])\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for key in dataset.keys():\n",
    "            acc_NP = accs_NP[key]\n",
    "            acc_full = accs_full[key]\n",
    "\n",
    "            df.loc[len(df.index)] = [acc_NP, acc_full, key]\n",
    "    \n",
    "    df_long = pd.melt(df, id_vars='dataset', value_vars=['NeuroPAL', 'Consolidated'], var_name='Accuracy', value_name='Value')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x='Accuracy', y='Value', data=df_long, color='skyblue', width=0.4)\n",
    "    sns.boxplot(x='Accuracy', y='Value', data=df_long, color='lightcoral', width=0.4)\n",
    "    sns.scatterplot(x='Accuracy', y='Value', data=df_long, color='skyblue')\n",
    "    sns.scatterplot(x='Accuracy', y='Value', data=df_long, color='lightcoral')\n",
    "\n",
    "    t, prob = stats.ttest_rel(np.asarray(df['NeuroPAL']), np.asarray(df['Consolidated']))\n",
    "\n",
    "    plt.ylabel('Assignment accuracy')\n",
    "    plt.xlabel('Atlas used')\n",
    "    plt.ylim((0,1))\n",
    "\n",
    "    print(t)\n",
    "    print(prob)\n",
    "    # Add lines connecting data points from the same dataset\n",
    "    #for i in range(len(df)):\n",
    "    #    plt.plot([i, i], [df['acc_NP'][i], df['acc_full'][i]], color='gray', linestyle='-', linewidth=1, alpha=0.7)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracies_atlas_compare([NP_dataset, chaud_dataset, Yem_dataset, old_FOCO_dataset, FOCO_dataset], accs_NP, accs_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file = '/Users/danielysprague/foco_lab/data/NP_Ray/20221215-20-02-49/full_comp.tif'\n",
    "data = skio.imread(raw_file)\n",
    "data = np.transpose(data)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "def plot_num_neur_heatmap(atlas, pairs, total_dataset):\n",
    "    neurons, num = get_neur_nums(total_dataset, atlas)\n",
    "\n",
    "    fig, axs = plt.subplots(1,3)\n",
    "    sns.set(style='white')\n",
    "\n",
    "    neur_df = atlas.df\n",
    "\n",
    "    neur_df = atlas.df[['ID', 'ganglion']]\n",
    "\n",
    "    dict_df = pd.DataFrame(list(neurons.items()), columns = ['ID', 'num'])\n",
    "    dict_df['frac'] = dict_df['num']/num\n",
    " \n",
    "    merged = pd.merge(neur_df, dict_df, on='ID') #this will preserve the order of neurons from neur_df which is sorted by ganglion and then distance along x axis\n",
    "\n",
    "    sns.barplot(ax=axs[0], y='ID', x='frac', hue='ganglion', data=merged, orient='h')\n",
    "    sns.despine(ax=axs[0], bottom=True, top=True,left=True, right=True)\n",
    "    axs[0].set_ylabel('Neuron IDs')\n",
    "    axs[0].set_xlabel('Fraction of datasets with ground truth labeled neuron')\n",
    "    axs[0].tick_params(labelleft=False, left=False)\n",
    "    bar_width = 0.8\n",
    "\n",
    "    for patch in axs[0].patches:\n",
    "        current_width = patch.get_height()\n",
    "        diff = current_width - bar_width\n",
    "        # Change the bar width\n",
    "        patch.set_height(bar_width)\n",
    "\n",
    "        # Recenter the bar\n",
    "        patch.set_y(patch.get_y() + diff * .5)\n",
    "\n",
    "    axs[0].invert_xaxis()\n",
    "    # Show the plot\n",
    "    axs[0].get_legend().remove()\n",
    "    #axs[0].legend(title='ganglion', loc='lower left')\n",
    "    \n",
    "    num_pair, num_heatmap, total_std_heatmap = analyze_pairs(pairs, neur_df, num)\n",
    "    ganglia_indices = {}\n",
    "\n",
    "    for ganglion in neur_df['ganglion'].unique():\n",
    "        # Find the indices where the category starts and ends\n",
    "        start_index = neur_df.index[neur_df['ganglion'] == ganglion][0]\n",
    "        end_index = neur_df.index[neur_df['ganglion'] == ganglion][-1]\n",
    "        \n",
    "        # Store the start and end indices in the dictionary\n",
    "        ganglia_indices[ganglion] = (start_index, end_index)\n",
    "\n",
    "    axs[1].set_facecolor('black')\n",
    "\n",
    "    mask = np.where(num_heatmap < 0.4, True, False)\n",
    "\n",
    "    sns.heatmap(data=total_std_heatmap, ax=axs[1], cmap='Reds', cbar = False)\n",
    "    highlight_boxes = [((ganglia_indices[gang][0], ganglia_indices[gang][0]), (ganglia_indices[gang][1], ganglia_indices[gang][1]), gang) for gang in neur_df['ganglion'].unique()]\n",
    "\n",
    "    for (x1, y1), (x2, y2), label in highlight_boxes:\n",
    "        axs[1].add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='black', lw=3))\n",
    "        if x1<len(neur_df)/2:\n",
    "            axs[1].text(x2+2, (y1 + y2) / 2, label, color='black', ha='left', va='center')\n",
    "        else:\n",
    "            axs[1].text(x1-2, (y1 + y2) / 2, label, color='black', ha='right', va='center')\n",
    "\n",
    "    axs[1].set_title('Standard deviation of pairwise distances')\n",
    "    axs[1].tick_params(which='both', bottom=False, left=False,labelbottom=False, labelleft=False)  # Hide tick labels\n",
    "\n",
    "    avg_std = np.sum(total_std_heatmap, axis=0)/total_std_heatmap.shape[0]\n",
    "\n",
    "    new_df = neur_df.copy()\n",
    "\n",
    "    new_df['std'] = avg_std\n",
    "\n",
    "    sns.barplot(ax=axs[2], data=new_df, y='ID', x='std', hue='ganglion',orient='h')\n",
    "    sns.despine(ax=axs[2], bottom=True, top=True,left=True, right=True)\n",
    "    axs[2].set_ylabel('')\n",
    "    axs[2].set_xlabel('Cumulative std in um')\n",
    "    axs[2].tick_params(labelleft=False, left=False)\n",
    "    axs[2].get_legend().remove()\n",
    "    #axs[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "\n",
    "    for patch in axs[2].patches:\n",
    "        current_width = patch.get_height()\n",
    "        diff = current_width - bar_width\n",
    "        # Change the bar width\n",
    "        patch.set_height(bar_width)\n",
    "\n",
    "        # Recenter the bar\n",
    "        patch.set_y(patch.get_y() + diff * .5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_num_neur_heatmap(atlas, pair_tot, tot_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synap_df = pd.read_csv('/Users/danielysprague/foco_lab/data/synaptic_connecs.csv')\n",
    "synap_df.head()\n",
    "\n",
    "df = pd.DataFrame(columns= ['Synaptic weight', 'LR pair', 'mean_dist', 'std_dist', 'Synapse type'])\n",
    "\n",
    "for i, row in synap_df.iterrows():\n",
    "    source = row['Source']\n",
    "    target = row['Target']\n",
    "    weight = row['Weight']\n",
    "    type = row['Type']\n",
    "\n",
    "    if source == target:\n",
    "        continue\n",
    "\n",
    "    if source[:-1] == target[:-1]:\n",
    "        LR = True\n",
    "    else:\n",
    "        LR = False\n",
    "\n",
    "    if source < target:\n",
    "        pair = source + '-' + target\n",
    "    else: \n",
    "        pair = target + '-' + source\n",
    "\n",
    "    if pair in pair_tot.keys():\n",
    "        dists = pair_tot[pair]\n",
    "        if len(dists) < 5:\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            mean = np.mean(dists)\n",
    "            std = np.std(dists)\n",
    "\n",
    "            df.loc[len(df)] = [weight, LR, mean, std, type]\n",
    "fig, axs = plt.subplots(1,2)\n",
    "\n",
    "sns.scatterplot(ax=axs[0], data=df[df['LR pair']==False], x='Synaptic weight', y='mean_dist', hue='Synapse type')\n",
    "axs[0].set_xlabel('synaptic weight between pair of neurons')\n",
    "axs[0].set_ylabel('mean distance in um')\n",
    "sns.scatterplot(ax=axs[1], data=df[df['LR pair']==False], x='Synaptic weight', y='std_dist', hue='Synapse type')\n",
    "axs[1].set_xlabel('synaptic weight between pair of neurons')\n",
    "axs[1].set_ylabel('standard deviation of distance in um')\n",
    "plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineage plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lin_dist_plot = lin_dist_df.loc[lin_dist_df['neuron1']!=lin_dist_df['neuron2']]\n",
    "lin_dist_plot = lin_dist_plot.loc[lin_dist_df['birth_dists']<800]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "fontsize=16\n",
    "\n",
    "tree_dists = np.asarray(lin_dist_plot['tree_dists'])\n",
    "std = np.asarray(lin_dist_plot['Std_dist'])\n",
    "\n",
    "#pearson_coef\n",
    "\n",
    "sns.regplot(data=lin_dist_plot, x='tree_dists',y='Std_dist')\n",
    "#sns.scatterplot(ax=axs[1], data=lin_dist_plot, x='lin_dist', y='Norm_std')\n",
    "plt.gca().invert_xaxis()\n",
    "plt.title('Standard deviation of pairwise physical distance and pairwise distance in cell lineage', fontsize=fontsize)\n",
    "plt.ylabel('Standard deviation in um', fontsize=fontsize)\n",
    "plt.xlabel('Lineal tree distance', fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "x = tree_dists.reshape((-1,1))\n",
    "y = std\n",
    "model = model.fit(x, y)\n",
    "\n",
    "r_sq = model.score(x,y)\n",
    "print('R2 value = '+ str(r_sq))\n",
    "print('y ='+str(model.coef_[0])+'*x + '+str(model.intercept_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_dist_plot = lin_dist_df.loc[lin_dist_df['neuron1']!=lin_dist_df['neuron2']]\n",
    "lin_dist_plot = lin_dist_plot.loc[lin_dist_df['birth_dists']<800]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "fontsize=16\n",
    "\n",
    "tree_dists = np.asarray(lin_dist_plot['tree_dists'])\n",
    "mean = np.asarray(lin_dist_plot['Mean_dist'])\n",
    "\n",
    "#pearson_coef\n",
    "\n",
    "sns.regplot(data=lin_dist_plot, x='tree_dists',y='Mean_dist')\n",
    "#sns.scatterplot(ax=axs[1], data=lin_dist_plot, x='lin_dist', y='Norm_std')\n",
    "plt.gca().invert_xaxis()\n",
    "plt.title('Mean of pairwise physical distance and pairwise distance in cell lineage', fontsize=fontsize)\n",
    "plt.ylabel('Mean distance in um', fontsize=fontsize)\n",
    "plt.xlabel('Lineal tree distance', fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "x = tree_dists.reshape((-1,1))\n",
    "y = mean\n",
    "model = model.fit(x, y)\n",
    "\n",
    "r_sq = model.score(x,y)\n",
    "print('R2 value = '+ str(r_sq))\n",
    "print('y ='+str(model.coef_[0])+'*x + '+str(model.intercept_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_dist_plot = lin_dist_df.loc[lin_dist_df['neuron1']!=lin_dist_df['neuron2']]\n",
    "lin_dist_plot = lin_dist_plot.loc[lin_dist_df['birth_dists']<800]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "fontsize=16\n",
    "\n",
    "birth_dists = np.asarray(lin_dist_plot['birth_dists'])\n",
    "std = np.asarray(lin_dist_plot['Std_dist'])\n",
    "\n",
    "#pearson_coef\n",
    "\n",
    "sns.regplot(data=lin_dist_plot, x='birth_dists',y='Std_dist')\n",
    "#sns.scatterplot(ax=axs[1], data=lin_dist_plot, x='lin_dist', y='Norm_std')\n",
    "plt.title('Standard deviation of pairwise physical distance and pairwise distance in cell lineage', fontsize=fontsize)\n",
    "plt.ylabel('Standard deviation in um', fontsize=fontsize)\n",
    "plt.xlabel('Time of birth of last shared parent cell between pair in min', fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "x = birth_dists.reshape((-1,1))\n",
    "y = std\n",
    "model = model.fit(x, y)\n",
    "\n",
    "r_sq = model.score(x,y)\n",
    "print('R2 value = '+ str(r_sq))\n",
    "print('y ='+str(model.coef_[0])+'*x + '+str(model.intercept_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_dist_plot = lin_dist_df.loc[lin_dist_df['neuron1']!=lin_dist_df['neuron2']]\n",
    "lin_dist_plot = lin_dist_plot.loc[lin_dist_df['birth_dists']<800]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "fontsize=16\n",
    "\n",
    "birth_dists = np.asarray(lin_dist_plot['birth_dists'])\n",
    "mean = np.asarray(lin_dist_plot['Mean_dist'])\n",
    "\n",
    "#pearson_coef\n",
    "\n",
    "sns.regplot(data=lin_dist_plot, x='birth_dists',y='Mean_dist')\n",
    "#sns.scatterplot(ax=axs[1], data=lin_dist_plot, x='lin_dist', y='Norm_std')\n",
    "plt.title('Mean of pairwise physical distance and pairwise distance in cell lineage', fontsize=fontsize)\n",
    "plt.ylabel('Mean distance in um', fontsize=fontsize)\n",
    "plt.xlabel('Time of birth of last shared parent cell between pair in min', fontsize=fontsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "x = birth_dists.reshape((-1,1))\n",
    "y = mean\n",
    "model = model.fit(x, y)\n",
    "\n",
    "r_sq = model.score(x,y)\n",
    "print('R2 value = '+ str(r_sq))\n",
    "print('y ='+str(model.coef_[0])+'*x + '+str(model.intercept_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_dist_plot = lin_dist_df.loc[lin_dist_df['neuron1']!=lin_dist_df['neuron2']]\n",
    "lin_dist_plot = lin_dist_df.loc[lin_dist_df['birth_dists']<800]\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "fontsize=16\n",
    "\n",
    "birth_dists = np.asarray(lin_dist_plot['birth_dists'])\n",
    "norm_std = np.asarray(lin_dist_plot['Norm_std'])\n",
    "\n",
    "#pearson_coef\n",
    "\n",
    "sns.scatterplot(data=lin_dist_plot, x='birth_dists',y='Mean_dist')\n",
    "#sns.scatterplot(ax=axs[1], data=lin_dist_plot, x='lin_dist', y='Norm_std')\n",
    "#plt.title('Mean of pairwise physical distance and pairwise distance in cell lineage', fontsize=fontsize)\n",
    "plt.ylabel('Mean distance in um', fontsize=fontsize)\n",
    "plt.xlabel('Time of birth of last shared parent cell between pair in min', fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['AntBulb-AntBulb', 'Ant-Ant', 'Dors-Dors', 'Lat-Lat', 'Vent-Vent', 'RVG-RVG', 'PostBulb-PostBulb', 'Ant-AntBulb', 'Ant-Dors', 'Ant-Lat', 'Dors-Lat', 'Lat-Vent','Lat-RVG','Lat-PostBulb', 'RVG-Vent', 'PostBulb-Vent','Postbulb-RVG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD Color statistics analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color_stats(folder):\n",
    "\n",
    "    rgbhist = np.zeros((32,3))\n",
    "\n",
    "    neur_colors = {}\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        if not file[-4:] == '.nwb':\n",
    "            continue\n",
    "\n",
    "        print(file)\n",
    "\n",
    "        blobs, rgb_data = get_nwb_neurons(folder+'/'+file)\n",
    "\n",
    "        color_norm = (rgb_data - np.min(rgb_data, axis=(0,1,2))) / (np.max(rgb_data, axis=(0,1,2))- np.min(rgb_data, axis=(0,1,2)))\n",
    "\n",
    "        blobs[['Rnorm', 'Gnorm','Bnorm']] = np.nan\n",
    "\n",
    "        for i, row in blobs.iterrows():\n",
    "            colors = color_norm[max(row['x']-2,0):min(row['x']+2,rgb_data.shape[0]-1),max(row['y']-2,0):min(row['y']+2,rgb_data.shape[1]-1),max(row['z']-1,0):min(row['z']+1,rgb_data.shape[2]-1),:]\n",
    "\n",
    "            flat_colors = colors.reshape(-1, colors.shape[-1])\n",
    "            \n",
    "            Rnorm = np.median(flat_colors[0])\n",
    "            Gnorm = np.median(flat_colors[1])\n",
    "            Bnorm = np.median(flat_colors[2])\n",
    "\n",
    "            blobs.loc[i, 'Rnorm'] = Rnorm\n",
    "            blobs.loc[i, 'Gnorm'] = Gnorm\n",
    "            blobs.loc[i, 'Bnorm'] = Bnorm\n",
    "\n",
    "        IDd = blobs[blobs['ID']!='']\n",
    "\n",
    "        for i, row in IDd.iterrows():\n",
    "            ID = row['ID']\n",
    "            colors = np.asarray(row[['Rnorm', 'Gnorm', 'Bnorm']])\n",
    "            if not ID in neur_colors:\n",
    "                neur_colors[ID] = [colors]\n",
    "            else:\n",
    "                neur_colors[ID].append(colors)\n",
    "            \n",
    "        image = np.asarray(color_norm)\n",
    "        im_flat = image.reshape(-1, image.shape[-1])\n",
    "\n",
    "        rhist, bins = np.histogram(im_flat[:,0], bins=32, range=(0,1))\n",
    "        ghist, bins = np.histogram(im_flat[:,1], bins=32, range=(0,1))\n",
    "        bhist, bins = np.histogram(im_flat[:,2], bins=32, range=(0,1))\n",
    "\n",
    "        rgbhist[:,0] += rhist\n",
    "        rgbhist[:,1] += ghist\n",
    "        rgbhist[:,2] += bhist\n",
    "    \n",
    "    rgbhist[:,0] = rgbhist[:,0]/np.sum(rgbhist[:,0])\n",
    "    rgbhist[:,1] = rgbhist[:,1]/np.sum(rgbhist[:,1])\n",
    "    rgbhist[:,2] = rgbhist[:,2]/np.sum(rgbhist[:,2])\n",
    "\n",
    "    return rgbhist, neur_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foco_rgb, foco_colors = get_color_stats('/Users/danielysprague/foco_lab/data/NWB_Ray')\n",
    "yem_rgb, yem_colors = get_color_stats('/Users/danielysprague/foco_lab/data/Yemini_NWB')\n",
    "foco_og_rgb, foco_og_colors = get_color_stats('/Users/danielysprague/foco_lab/data/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(dataset_rgbs, labels):\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "    fig, axs = plt.subplots(1,3, sharey=True)\n",
    "\n",
    "    # Plot the bars using Matplotlib\n",
    "\n",
    "    bin_edges = np.arange(32)/32\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        #ax.hist(im_flat[:,i], bins= np.arange(32)/32, color= 'red')\n",
    "        for j, dataset in enumerate(dataset_rgbs):\n",
    "            ax.bar(bin_edges[:], dataset[:,i], alpha=0.5, align='center', width=1/32, color=sns.color_palette(\"pastel\")[j], log=True, label=labels[j])\n",
    "            #ax.bar(bin_edges[:], yem_rgb[:,0], alpha=0.5, align='center', width=1/32, color=sns.color_palette(\"pastel\")[1], log=True, label='Yemini')\n",
    "        ax.legend()\n",
    "        #sns.histplot(ax=ax, data=np.ones(32),weights=np.transpose(foco_rgb[:,i]), bins= np.arange(32)/32, alpha=0.3, stat = 'probability', log_scale=(False,True), label='FOCO')\n",
    "        #sns.histplot(ax=ax, data=np.ones(32),weights=np.transpose(yem_rgb[:,i]), bins=np.arange(32)/32, alpha=0.3, stat = 'probability',log_scale=(False,True), label='Yemini')\n",
    "\n",
    "    axs[0].set_title('Red histogram')\n",
    "    axs[0].set_xlabel('Normalized color')\n",
    "    axs[0].set_ylabel('log probability')\n",
    "    axs[1].set_title('Green histogram')\n",
    "    axs[1].set_xlabel('Normalized color')\n",
    "    axs[2].set_title('Blue histogram')\n",
    "    axs[2].set_xlabel('Normalized color')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_histograms([foco_rgb, yem_rgb], ['FOCO', 'Yemini'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_discrim_FOCO = get_color_discrim('/Users/danielysprague/foco_lab/data/NWB_Ray', 6)\n",
    "color_discrim_Yemini = get_color_discrim('/Users/danielysprague/foco_lab/data/Yemini_NWB', 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df = pd.DataFrame(columns=['avg_col_discrim', 'identifier'])\n",
    "\n",
    "datasets= [color_discrim_FOCO, color_discrim_Yemini, color_discrim_FOCO_og, color_discrim_NP]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for key, value in dataset.items():\n",
    "        color_df = pd.concat([color_df, pd.DataFrame({'avg_col_discrim': np.mean(value), 'identifier':key[:-4]}, index=[0])], ignore_index=True)\n",
    "\n",
    "print(color_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_color_discrim([color_discrim_FOCO, color_discrim_Yemini, color_discrim_FOCO_og, color_discrim_NP], ['FOCO', 'Yemini', 'FOCO_og', 'NP'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
