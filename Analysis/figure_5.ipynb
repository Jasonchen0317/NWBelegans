{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "import os\n",
    "\n",
    "from adjustText import adjust_text\n",
    "import colorsys\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import tz\n",
    "from hdmf.backends.hdf5.h5_utils import H5DataIO\n",
    "from hdmf.container import Container\n",
    "from hdmf.data_utils import DataChunkIterator\n",
    "import latex\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pynwb import load_namespaces, get_class, register_class, NWBFile, TimeSeries, NWBHDF5IO\n",
    "from pynwb.file import MultiContainerInterface, NWBContainer, Device, Subject\n",
    "from pynwb.ophys import ImageSeries, OnePhotonSeries, OpticalChannel, ImageSegmentation, PlaneSegmentation, Fluorescence, DfOverF, CorrectedImageStack, MotionCorrection, RoiResponseSeries, ImagingPlane\n",
    "from pynwb.core import NWBDataInterface\n",
    "from pynwb.epoch import TimeIntervals\n",
    "from pynwb.behavior import SpatialSeries, Position\n",
    "from pynwb.image import ImageSeries\n",
    "import pywt\n",
    "import scipy.io as sio\n",
    "import scipy\n",
    "from scipy.stats import multivariate_normal, spearmanr\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import seaborn as sns\n",
    "import skimage.io as skio\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from tifffile import TiffFile\n",
    "import tifffile\n",
    "\n",
    "from networkx import kamada_kawai_layout\n",
    "\n",
    "from atlas import loadmat, NPAtlas, NWBAtlas\n",
    "from process_file import get_nwb_neurons, get_dataset_neurons, get_dataset_online, combine_datasets, get_pairings, get_color_discrim, get_neur_nums\n",
    "from stats import get_summary_stats, analyze_pairs, get_accuracy\n",
    "from visualization import plot_num_heatmap, plot_std_heatmap, plot_summary_stats, plot_color_discrim, plot_accuracies, plot_visualizations_atlas, plot_visualizations_data, plot_atlas2d_super\n",
    "from utils import covar_to_coord, convert_coordinates, maha_dist, run_linear_assignment\n",
    "\n",
    "# ndx_mulitchannel_volume is the novel NWB extension for multichannel optophysiology in C. elegans\n",
    "#from ndx_multichannel_volume import CElegansSubject, OpticalChannelReferences, OpticalChannelPlus, ImagingVolume, VolumeSegmentation, MultiChannelVolume, MultiChannelVolumeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional import if you want to open figures in a separate window, add %matplotlib qt to top of any code box if you want figures to open in a separate window \n",
    "import PyQt6.QtCore\n",
    "os.environ[\"QT_API\"] = \"pyqt6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5: Benchmarking atlas performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 5 demonstrates the results of benchmarking three different atlases on the full corpus of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = NWBAtlas(atlas_file = '../data/atlases/2024_03_11_match_full_nosplit.pkl') # Load atlas\n",
    "atlas_df = atlas.get_df()\n",
    "atlas_neurons = np.asarray(atlas_df['ID'])\n",
    "atlas.df = atlas.df.drop(atlas.df[atlas.df['ID']=='IL1V'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Can either load datasets using get_dataset_neurons with path to a folder of NWB files or using get_dataset_online to stream in data from an online dandiset.\n",
    "Note that streaming from online will take much longer to load here but bypasses the need to have any files stored on your local hard drive.\n",
    "'''\n",
    "\n",
    "data_path = 'insert path to folder containing NWB files to load or stream from online'\n",
    "#dataset = get_dataset_neurons(data_path, atlas_neurons)\n",
    "\n",
    "EY_dataset = get_dataset_online('000541', atlas_neurons) #dandi_id = 000541\n",
    "SK1_dataset = get_dataset_online('000565', atlas_neurons) #dandi_id = 000565\n",
    "NP_dataset = get_dataset_online('000715', atlas_neurons) #dandi_id = 000715\n",
    "SK2_dataset = get_dataset_online('000472', atlas_neurons) #dandi_id = 000472\n",
    "HL_dataset = get_dataset_online('000714', atlas_neurons) #dandi_id = 000714\n",
    "KK_dataset = get_dataset_online('000692', atlas_neurons) #dandi_id = 000692\n",
    "SF_dataset = get_dataset_online('000776', atlas_neurons) #dandi_id = 000776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maha_dist(data, mu, sigma):\n",
    "\n",
    "    data_mu = data-mu\n",
    "    inv_sigma = np.linalg.inv(sigma)\n",
    "    left_data = np.dot(data_mu, inv_sigma)\n",
    "    mahal = np.dot(left_data, data_mu.T)\n",
    "\n",
    "    return np.sqrt(mahal)\n",
    "\n",
    "def run_linear_assignment(df_data, atlas):\n",
    "    #df_data should have columns 'X', 'Y', 'Z', 'R', 'G', 'B', 'ID' \n",
    "\n",
    "    df_assigns = df_data.copy()\n",
    "\n",
    "    mu = atlas.mu\n",
    "    sigma = atlas.sigma\n",
    "    neurons = np.asarray(atlas.neurons)\n",
    "\n",
    "    xyzrgb = np.asarray(df_data[['X','Y','Z','R','G','B']])\n",
    "    gt_labels = np.asarray(df_data['ID'])\n",
    "\n",
    "    assigns = np.empty((xyzrgb.shape[0],5),np.dtype('U100'))\n",
    "    assign_cost = np.zeros((xyzrgb.shape[0], 3, 5)) #total, position, color in second dimension, top 5 ranks in third \n",
    "\n",
    "    cost_mat = np.zeros((xyzrgb.shape[0], mu.shape[0]))\n",
    "    cost_pos = np.zeros((xyzrgb.shape[0], mu.shape[0]))\n",
    "    cost_col = np.zeros((xyzrgb.shape[0], mu.shape[0]))\n",
    "\n",
    "    for i in range(xyzrgb.shape[0]):\n",
    "        for j in range(mu.shape[0]):\n",
    "            cost = maha_dist(xyzrgb[i,:], mu[j,:], sigma[:,:,j])\n",
    "            cost_pos[i,j] = maha_dist(xyzrgb[i,:3], mu[j,:3], sigma[:3,:3,j])\n",
    "            cost_col[i,j] = maha_dist(xyzrgb[i,3:], mu[j,3:], sigma[3:,3:,j])\n",
    "\n",
    "            cost_mat[i,j] = cost\n",
    "\n",
    "    for k in range(5):\n",
    "\n",
    "        row_inds, col_inds = linear_sum_assignment(cost_mat)\n",
    "\n",
    "        assigns[row_inds,k] = np.asarray(neurons[col_inds])\n",
    "\n",
    "        assign_cost[row_inds, 0, k] = cost_mat[row_inds, col_inds]\n",
    "        assign_cost[row_inds, 1, k] = cost_pos[row_inds, col_inds]\n",
    "        assign_cost[row_inds, 2, k] = cost_col[row_inds, col_inds]\n",
    "\n",
    "        cost_mat[row_inds, col_inds] = np.inf\n",
    "\n",
    "    df_assigns['assign_1'] = assigns[:,0]\n",
    "    df_assigns['assign_2'] = assigns[:,1]\n",
    "    df_assigns['assign_3'] = assigns[:,2]\n",
    "    df_assigns['assign_4'] = assigns[:,3]\n",
    "    df_assigns['assign_5'] = assigns[:,4]\n",
    "\n",
    "    return df_assigns, assign_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies(folder, atlas):\n",
    "    acc_df = pd.DataFrame(columns=['Total_neurons','Percent_IDd', 'Percent_top1', 'Percent_top2', 'Percent_top3', 'Percent_top4', 'Percent_top5', 'Filename'])\n",
    "    for file in os.listdir(folder):\n",
    "        if not file[-4:] == '.csv':\n",
    "            continue\n",
    "\n",
    "        df_data = pd.read_csv(folder + '/'+file)\n",
    "        df_data = df_data.rename(columns={\"aligned_x\":\"X\",\"aligned_y\":\"Y\",\"aligned_z\":\"Z\", \"aligned_R\":\"R\", \"aligned_G\":\"G\", \"aligned_B\":\"B\"})\n",
    "\n",
    "        df, costs = run_linear_assignment(df_data, atlas)\n",
    "\n",
    "        IDd = df[~df['ID'].isnull()]\n",
    "\n",
    "        per_ID = len(IDd.index)/len(df.index)\n",
    "\n",
    "        total_neurons = len(df.index)\n",
    "\n",
    "        corr1 = df.loc[df['ID']==df['assign_1']]\n",
    "        corr2 = df.loc[df['ID']==df['assign_2']]\n",
    "        corr3 = df.loc[df['ID']==df['assign_3']]\n",
    "        corr4 = df.loc[df['ID']==df['assign_4']]\n",
    "        corr5 = df.loc[df['ID']==df['assign_5']]\n",
    "            \n",
    "        corr_cum_2 = pd.concat([corr1,corr2]).drop_duplicates().reset_index(drop=True)\n",
    "        corr_cum_3 = pd.concat([corr_cum_2,corr3]).drop_duplicates().reset_index(drop=True)\n",
    "        corr_cum_4 = pd.concat([corr_cum_3,corr4]).drop_duplicates().reset_index(drop=True)\n",
    "        corr_cum_5 = pd.concat([corr_cum_4, corr5]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        per_corr_1 = len(corr1.index)/len(IDd.index)\n",
    "        per_corr_2 = len(corr_cum_2.index)/len(IDd.index)\n",
    "        per_corr_3 = len(corr_cum_3.index)/len(IDd.index)\n",
    "        per_corr_4 = len(corr_cum_4.index)/len(IDd.index)\n",
    "        per_corr_5 = len(corr_cum_5.index)/len(IDd.index)\n",
    "\n",
    "        acc_df.loc[len(acc_df.index)] = [total_neurons,per_ID, per_corr_1, per_corr_2, per_corr_3, per_corr_4, per_corr_5, file[:-4]]\n",
    "\n",
    "    return acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get accuracy values for each dataset using the trained atlas and the roughly aligned point clouds. If you would like\n",
    "to test on datasets that have not been pre-aligned, please use the neuroPAL_ID software which has the atlas and alignment\n",
    "code pre-compiled\n",
    "'''\n",
    "\n",
    "NP_atlas_match = NWBAtlas(atlas_file = '/Users/danielysprague/foco_lab/data/atlases/2024_03_11_NPonly.pkl') #Atlas trained on just original 10 NeuroPAL datasets\n",
    "NP_atlas_unmatch = NWBAtlas(atlas_file = '/Users/danielysprague/foco_lab/data/atlases/2024_03_11_NPunmatch.pkl')\n",
    "\n",
    "accs_NP = get_accuracies('/Users/danielysprague/foco_lab/data/aligned_2024_03_11/aligned_NP', NP_atlas_match)\n",
    "accs_NP_unmatch = get_accuracies('/Users/danielysprague/foco_lab/data/aligned_2024_03_11/aligned_NP_nomatch', NP_atlas_unmatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    full_atlas_match = NWBAtlas(atlas_file = '/Users/danielysprague/foco_lab/data/atlases/2024_03_11_split/exgroup'+str(i)+'.pkl') \n",
    "    full_atlas_unmatch = NWBAtlas(atlas_file = '/Users/danielysprague/foco_lab/data/atlases/2024_03_11_split_unmatch/exgroup'+str(i)+'.pkl')\n",
    "\n",
    "    accs_match = get_accuracies('/Users/danielysprague/foco_lab/data/aligned_2024_03_11/aligned_split/group'+str(i+1), full_atlas_match)\n",
    "    accs_unmatch = get_accuracies('/Users/danielysprague/foco_lab/data/aligned_2024_03_11/aligned_split_nomatch/group'+str(i+1), full_atlas_unmatch)\n",
    "\n",
    "    if i==0:\n",
    "        accs_full = accs_match\n",
    "        accs_full_unmatch = accs_unmatch\n",
    "    else:\n",
    "        accs_full = pd.concat([accs_full, accs_match])\n",
    "        accs_full_unmatch = pd.concat([accs_full_unmatch, accs_unmatch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping files that have obvious artifacts or known alignment issues\n",
    "skipfiles = ['20231013-9-30-0', '20230412-20-15-17', '2023-01-23-01', '20239828-11-14-0', '2023-01-05-01', '2023-01-10-14', '2022-06-28-07', '2022-07-26-01', '2023-01-19-15', '2022-07-15-06', '2022-08-02-01', '2023-01-09-08', '2023-01-09-28', '2023-01-10-14', '2023-01-17-14', '2023-01-19-22', '2023-01-23-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_plots_acc(datasets, labels, skipfiles, accs_NP_unmatch, accs_NP, accs_full_unmatch, accs_full):\n",
    "\n",
    "    plt.rcParams.update({'font.size': 60})\n",
    "    sns.set(style='white', font_scale=1.5)\n",
    "\n",
    "    df_dataset = pd.DataFrame(columns=['Atlas', 'Dataset', 'Accuracy'])\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for key in dataset.keys():\n",
    "            if key in skipfiles: #skip files\n",
    "                continue\n",
    "            acc_NP_unmatch = accs_NP_unmatch.loc[accs_NP_unmatch['Filename']==key]\n",
    "            acc_NP = accs_NP.loc[accs_NP['Filename']==key]\n",
    "            acc_full_unmatch = accs_full_unmatch.loc[accs_full_unmatch['Filename']==key]\n",
    "            acc_full = accs_full.loc[accs_full['Filename']==key]\n",
    "\n",
    "            df_dataset.loc[len(df_dataset.index)] = ['Base', labels[i], acc_NP_unmatch.iloc[0]['Percent_top1']]\n",
    "            df_dataset.loc[len(df_dataset.index)] = ['Matched', labels[i], acc_NP.iloc[0]['Percent_top1']]\n",
    "            df_dataset.loc[len(df_dataset.index)] = ['Full', labels[i], acc_full_unmatch.iloc[0]['Percent_top1']]\n",
    "            df_dataset.loc[len(df_dataset.index)] = ['Full matched', labels[i], acc_full.iloc[0]['Percent_top1']]\n",
    "\n",
    "    df_ranks = pd.DataFrame(columns= ['Atlas', 'Rank', 'Accuracy'])\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for key in dataset.keys():\n",
    "            if key in skipfiles:\n",
    "                continue\n",
    "            acc_NP_unmatch = accs_NP_unmatch.loc[accs_NP_unmatch['Filename']==key]\n",
    "            acc_NP = accs_NP.loc[accs_NP['Filename']==key]\n",
    "            acc_full_unmatch = accs_full_unmatch.loc[accs_full_unmatch['Filename']==key]\n",
    "            acc_full = accs_full.loc[accs_full['Filename']==key]\n",
    "\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Original', 'top', acc_NP_unmatch.iloc[0]['Percent_top1']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Color corrected', 'top', acc_NP.iloc[0]['Percent_top1']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Multi-lab', 'top', acc_full_unmatch.iloc[0]['Percent_top1']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Multi-lab + color corrected', 'top', acc_full.iloc[0]['Percent_top1']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Original', 'top2', acc_NP_unmatch.iloc[0]['Percent_top2']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Color corrected', 'top2', acc_NP.iloc[0]['Percent_top2']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Multi-lab', 'top2', acc_full_unmatch.iloc[0]['Percent_top2']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Multi-lab + color corrected', 'top2', acc_full.iloc[0]['Percent_top2']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Original', 'top3', acc_NP_unmatch.iloc[0]['Percent_top3']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Color corrected', 'top3', acc_NP.iloc[0]['Percent_top3']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Multi-lab', 'top3', acc_full_unmatch.iloc[0]['Percent_top3']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Multi-lab + color corrected', 'top3', acc_full.iloc[0]['Percent_top3']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Original', 'top4', acc_NP_unmatch.iloc[0]['Percent_top4']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Color corrected', 'top4', acc_NP.iloc[0]['Percent_top4']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Multi-lab', 'top4', acc_full_unmatch.iloc[0]['Percent_top4']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Multi-lab + color corrected', 'top4', acc_full.iloc[0]['Percent_top4']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Original', 'top5', acc_NP_unmatch.iloc[0]['Percent_top4']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Color corrected', 'top5', acc_NP.iloc[0]['Percent_top4']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Multi-lab', 'top5', acc_full_unmatch.iloc[0]['Percent_top4']]\n",
    "            df_ranks.loc[len(df_ranks.index)] = ['Multi-lab + color corrected', 'top5', acc_full.iloc[0]['Percent_top4']]\n",
    "\n",
    "    palette = sns.color_palette('colorblind')\n",
    "    color1 = palette[3]\n",
    "    color2 = palette[2]\n",
    "    color3 = palette[0]\n",
    "    color4 = palette[8]\n",
    "    color5 = palette[4]\n",
    "    color6 = palette[6]\n",
    "\n",
    "    axs = plt.figure(layout=\"constrained\").subplot_mosaic(\n",
    "        \"\"\"\n",
    "        AB\n",
    "        AC\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    for key in axs.keys():\n",
    "        axs[key].spines[['right', 'top']].set_visible (False)\n",
    "        axs[key].axhline(1.0, ls='--', c='grey')\n",
    "        axs[key].axhline(0.75, ls='--', c='grey')\n",
    "        axs[key].axhline(0.5, ls='--', c='grey')\n",
    "        axs[key].axhline(0.25, ls='--', c='grey')\n",
    "\n",
    "    # Calculate mean and standard error of the mean for each category\n",
    "    df_nofull = df_dataset[df_dataset['Atlas']!='Full']\n",
    "    stats = df_nofull.groupby('Atlas')['Accuracy'].agg(['mean', 'sem']).reset_index()\n",
    "\n",
    "    sns.violinplot(ax = axs['A'], data=df_dataset[df_dataset['Atlas']!='Full'], x='Atlas', y='Accuracy', hue='Atlas', palette=[color1, color2, color4], cut=0, inner='point', density_norm='width', inner_kws = {})\n",
    "\n",
    "    for i, row in stats.iterrows():\n",
    "        cat_index = np.where(df_nofull['Atlas'].unique() == row['Atlas'])[0][0]\n",
    "        axs['A'].axhline(y=row['mean'], color='red', linestyle='-', linewidth=3, xmin=cat_index/len(df_nofull['Atlas'].unique()), xmax=(cat_index+1)/len(df_nofull['Atlas'].unique()))\n",
    "\n",
    "    sns.violinplot(ax= axs['B'], data = df_ranks[(df_ranks['Atlas']=='Original') | (df_ranks['Atlas']=='Multi-lab + color corrected')], x = 'Rank', y='Accuracy', hue='Atlas', gap=0.5, palette=[color1, color4], orient='v', split=True, cut=0, inner='quart', density_norm='width')\n",
    "    sns.violinplot(ax= axs['C'], data = df_dataset[(df_dataset['Atlas']=='Base')|(df_dataset['Atlas']=='Full matched')], x = 'Dataset', y='Accuracy', hue='Atlas', gap=0.5, palette=[color1, color4], orient='v', split=True, cut=0, inner='quart', density_norm='width') \n",
    "    \n",
    "    accs_base = np.asarray(df_dataset[df_dataset['Atlas'] == 'Base']['Accuracy'])\n",
    "    accs_match = np.asarray(df_dataset[df_dataset['Atlas'] == 'Matched']['Accuracy'])\n",
    "    accs_fullnomatch = np.asarray(df_dataset[df_dataset['Atlas'] == 'Full']['Accuracy'])\n",
    "    accs_fullmatch = np.asarray(df_dataset[df_dataset['Atlas'] == 'Full matched']['Accuracy'])\n",
    "\n",
    "    for i, dataset in enumerate(labels):\n",
    "        NP_unmatch_vals = df_dataset[(df_dataset['Dataset']==dataset)&(df_dataset['Atlas']=='Base')]['Accuracy']\n",
    "        consol_vals = df_dataset[(df_dataset['Dataset']==dataset)&(df_dataset['Atlas']=='Full matched')]['Accuracy']\n",
    "\n",
    "        for a, b in zip(NP_unmatch_vals, consol_vals):\n",
    "            axs['C'].plot([i-0.1,i+0.1], [a,b], color='black', linewidth=0.5)\n",
    "\n",
    "    ranks = ['top', 'top2', 'top3', 'top4', 'top5']\n",
    "\n",
    "    for j, rank in enumerate(ranks):\n",
    "        NP_vals_rank = df_ranks[(df_ranks['Rank']==rank)&(df_ranks['Atlas']=='Original')]['Accuracy']\n",
    "        consol_match_vals_rank = df_ranks[(df_ranks['Rank']==rank)&(df_ranks['Atlas']=='Multi-lab + color corrected')]['Accuracy']\n",
    "\n",
    "        for a, b in zip(NP_vals_rank, consol_match_vals_rank):\n",
    "            axs['B'].plot([j-0.1,j+0.1], [a,b], color='black', linewidth=0.2, label = '_nolegend_')\n",
    "\n",
    "    axs['A'].set_ylim((0,1))\n",
    "    axs['A'].set(xlabel=None)\n",
    "    axs['A'].set_title('Atlas performance comparison')\n",
    "    axs['A'].set_yticks([0,0.25,0.5,0.75,1.0])\n",
    "    axs['A'].set_xticklabels([])\n",
    "    axs['B'].set_ylim((0,1))\n",
    "    axs['B'].legend([],[], frameon=False)\n",
    "    axs['B'].set_xlabel(None)\n",
    "    axs['B'].set_title('Accuracy by rank')\n",
    "    axs['B'].set_yticks([0,0.25,0.5,0.75,1.0])\n",
    "    axs['C'].set_ylim((0,1))\n",
    "    axs['C'].legend([],[], frameon=False)\n",
    "    axs['C'].set_xlabel(None)\n",
    "    axs['C'].set_title('Accuracy by dataset')\n",
    "    axs['C'].set_yticks([0,0.25,0.5,0.75,1.0])\n",
    "\n",
    "\n",
    "    base_accs = np.asarray(df_dataset[df_dataset['Atlas']=='Base']['Accuracy'])\n",
    "    match_accs = np.asarray(df_dataset[df_dataset['Atlas']=='Matched']['Accuracy'])\n",
    "    consol_accs = np.asarray(df_dataset[df_dataset['Atlas']=='Full']['Accuracy'])\n",
    "    consol_match_accs = np.asarray(df_dataset[df_dataset['Atlas']=='Full matched']['Accuracy'])\n",
    "\n",
    "    base_match = scipy.stats.ttest_rel(base_accs, match_accs)\n",
    "    base_fullmatch = scipy.stats.ttest_rel(base_accs, consol_match_accs)\n",
    "    match_fullmatch = scipy.stats.ttest_rel(match_accs, consol_match_accs)\n",
    "\n",
    "    print('t-value: ' +str(base_match.statistic)+' pvalue: '+str(base_match.pvalue))\n",
    "    print('t-value: ' +str(base_fullmatch.statistic)+' pvalue: '+str(base_fullmatch.pvalue))\n",
    "    print('t-value: ' +str(match_fullmatch.statistic)+' pvalue: '+str(match_fullmatch.pvalue))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return accs_base, accs_match, accs_fullnomatch, accs_fullmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['EY', 'HL', 'KK', 'SF', 'SK1', 'SK2']\n",
    "datasets = [EY_dataset,HL_dataset, KK_dataset, SF_dataset, SK1_dataset, SK2_dataset]\n",
    "\n",
    "accs_base, accs_match, accs_full_nomatch, accs_fullmatch = gen_plots_acc(datasets, labels, skipfiles, accs_NP_unmatch, accs_NP, accs_full_unmatch, accs_full)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
